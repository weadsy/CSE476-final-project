{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "def load_save_json(path_in=\"parsed_dev_data.json\", path_out=None, data_in=None, clear=False):\n",
    "    data = json.load(open(path_in, \"r\", encoding=\"utf-8\")) if not clear else []\n",
    "    if path_out is not None:\n",
    "        data.append(data_in)\n",
    "        with open(path_out, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "            \n",
    "    return data\n",
    "            \n",
    "all_tests = load_save_json()\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_json(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf'), seed=None, index=None):\n",
    "    if index is not None: return all_tests[index]\n",
    "    \n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if seed is not None: random.seed(seed)\n",
    "    \n",
    "    if n == 0:\n",
    "        return filtered_tests[start:end]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 440\n",
      "['First',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'many',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'the',\n",
      " 'original',\n",
      " 'bolt',\n",
      " 'of',\n",
      " 'fabric',\n",
      " 'was:',\n",
      " '16',\n",
      " 'feet',\n",
      " '*',\n",
      " '12',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<16*12=192>>192',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Then',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'much',\n",
      " 'fabric',\n",
      " 'Ann',\n",
      " 'took',\n",
      " 'for',\n",
      " 'the',\n",
      " 'living',\n",
      " 'room',\n",
      " 'curtains:',\n",
      " '4',\n",
      " 'feet',\n",
      " '*',\n",
      " '6',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<4*6=24>>24',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Then',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'much',\n",
      " 'fabric',\n",
      " 'Ann',\n",
      " 'took',\n",
      " 'for',\n",
      " 'the',\n",
      " 'bathroom',\n",
      " 'curtains:',\n",
      " '2',\n",
      " 'feet',\n",
      " '*',\n",
      " '4',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<2*4=8>>8',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Finally,',\n",
      " 'subtract',\n",
      " 'the',\n",
      " 'square',\n",
      " 'footage',\n",
      " 'of',\n",
      " 'both',\n",
      " 'sets',\n",
      " 'of',\n",
      " 'curtains',\n",
      " 'from',\n",
      " 'the',\n",
      " 'total',\n",
      " 'square',\n",
      " 'footage:',\n",
      " '192',\n",
      " '-',\n",
      " '24',\n",
      " '-',\n",
      " '8',\n",
      " '=',\n",
      " '<<192-24-8=160>>160',\n",
      " 'square',\n",
      " 'feet',\n",
      " '####',\n",
      " '160']\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=1, upper_char=300) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output, exp):\n",
    "    exp = str(exp)\n",
    "    exp = exp.lower().strip('.')\n",
    "    out = output.lower().strip('.')\n",
    "    \n",
    "    #rare case when exp is actually yes/now and model output is true/false\n",
    "    if exp == \"yes\" and out == \"true\": return \"yes\"\n",
    "    if exp == \"no\" and out == \"false\": return \"no\"\n",
    "    \n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output, exp)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "c023deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_matches(toCount, toMatch):\n",
    "    counter = Counter(toCount)\n",
    "    match_counts = {word: counter.get(word, 0) for word in toMatch}\n",
    "    total_matches = sum(match_counts.values())\n",
    "    output_len = len(toCount)\n",
    "    #print(f\"{total_matches}/{output_len} : {(total_matches / output_len) * 100 if output_len != 0 else 0}%\")\n",
    "    #print('match counts:', match_counts)\n",
    "    return total_matches, output_len\n",
    "\n",
    "def get_cosine(expected_counter, output_counter):\n",
    "    dot_product = sum(expected_counter[word] * output_counter.get(word, 0) for word in expected_counter)\n",
    "    \n",
    "    #print(f\"Dot product: {dot_product}\")\n",
    "    \n",
    "    exp_mag = math.sqrt(sum(v**2 for v in expected_counter.values()))\n",
    "    out_mag = math.sqrt(sum(v**2 for v in output_counter.values()))\n",
    "    \n",
    "    cosine_sim = 0\n",
    "    if exp_mag > 0 and out_mag > 0:\n",
    "        cosine_sim = dot_product / (exp_mag * out_mag)\n",
    "        print(f\"\\n[Cosine similarity: {cosine_sim}]\")\n",
    "        \n",
    "    return cosine_sim\n",
    "\n",
    "def get_start_end_matches(expected, output, exp_len, out_len):\n",
    "    start_matches = False\n",
    "    end_matches = False\n",
    "    if expected[0] in output[0]: start_matches = True\n",
    "    if expected[exp_len-1] in output[out_len-1]: end_matches = True\n",
    "    #print('exp', expected)\n",
    "    #print('output', output)\n",
    "    \n",
    "    #print(f\"expected[0] {expected[0]}, output[0] {output[0]}\")\n",
    "    #print(f\"expected[exp_len-1] {expected[exp_len-1]}, output[out_len-1] {output[out_len-1]}\")\n",
    "    #print(f\"START {start_matches} END {end_matches}\")\n",
    "    \n",
    "    return start_matches, end_matches\n",
    "    \n",
    "def super_match(test, output):\n",
    "    expected = str(test[\"expected\"]).replace('$', '').lower().split()\n",
    "    output = output.replace('$', '').lower().split()\n",
    "    \n",
    "    expected_counter = Counter(expected)\n",
    "    output_counter = Counter(output)\n",
    "    \n",
    "    #not very helpful in the long run...\n",
    "    get_cosine(expected_counter, output_counter)\n",
    "    \n",
    "    exp_matches, out_len = create_matches(output, expected)\n",
    "    out_matches, exp_len = create_matches(expected, output)\n",
    "    \n",
    "    return get_start_end_matches(expected, output, exp_len, out_len)\n",
    "    #return match_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    test_samples = {\n",
    "        \"count\": len(tests),\n",
    "        \"seed\": None,\n",
    "        \"samples\": None\n",
    "    }\n",
    "    \n",
    "    for t in tests:\n",
    "        sample = {\n",
    "            \"test_count\": count,\n",
    "            \"id\": t[\"id\"],\n",
    "            \"input\": t['prompt'],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": None,\n",
    "            \"history\": {\n",
    "                \"check_correct1\": {\n",
    "                    \"match_check\": None,\n",
    "                    \"self_eval\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"truncated\": False,\n",
    "                \"check_correct2\": {\n",
    "                    \"self_eval\": None,\n",
    "                    \"self_eval2\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"check_correct3\": {\n",
    "                    \"self_eval2\": None,\n",
    "                    \"sides_matching\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"final_correctness\": None\n",
    "            }\n",
    "        }\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_json(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        sample[\"got\"]=got\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got, t[\"expected\"])\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        if not not_truncated:\n",
    "            final_answers.append(False)\n",
    "            sample['truncated'] = True\n",
    "            print(\"‚ùå INCORRECT | MAX TOKENS REACHED RETURNING FALSE\")\n",
    "            continue\n",
    "        \n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        sample[\"history\"][\"check_correct1\"][\"match_check\"] = match_check\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        sample[\"history\"][\"check_correct1\"][\"self_eval\"] = is_correct\n",
    "        \n",
    "        seperator('\\nMODEL OUTPUT --> FIRST EVAL')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        sample[\"history\"][\"check_correct1\"]['correctness'] = correctness\n",
    "        sample[\"history\"][\"check_correct1\"]['agreement'] = agreement\n",
    "        \n",
    "        if not agreement:\n",
    "            #starting and ending matches\n",
    "            #CAN BE USED TO VALIDATE SECOND MODEL, OR AS LAST RESORT\n",
    "            start_matches, end_matches = super_match(t, got)\n",
    "            sides_matching = start_matches or end_matches\n",
    "            \n",
    "            #second model eval\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval\"] = is_correct\n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval2\"] = is_correct2\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            sample[\"history\"][\"check_correct2\"][\"correctness\"] = correctness\n",
    "            sample[\"history\"][\"check_correct2\"][\"agreement\"] = agreement\n",
    "            \n",
    "            \n",
    "            if not agreement:\n",
    "                #second model eval\n",
    "                seperator('\\nDISAGREEMENT --> THIRD EVAL')\n",
    "                print('\\nside matching:', sides_matching)\n",
    "                \n",
    "                sample[\"history\"][\"check_correct3\"][\"self_eval2\"] = is_correct2\n",
    "                sample[\"history\"][\"check_correct3\"][\"sides_matching\"] = sides_matching\n",
    "                correctness, agreement = check_correct(sides_matching, is_correct2)\n",
    "                sample[\"history\"][\"check_correct3\"][\"correctness\"] = correctness\n",
    "                sample[\"history\"][\"check_correct3\"][\"agreement\"] = agreement    \n",
    "\n",
    "\n",
    "        sample[\"history\"][\"final_correctness\"] = f\"‚úÖ {correctness}\" if correctness else f\"‚ùå {correctness}\"\n",
    "        final_answers.append(sample)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    test_samples[\"samples\"] = final_answers\n",
    "    return test_samples\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"future_prediction_2_5_505\",\n",
      "  \"type\": \"future_prediction\",\n",
      "  \"prompt\": \"You are an agent that can predict future events. The event to be predicted: \\\"Volta Redonda vs. Novorizontino (around 2025-08-10T07:30:00Z). \\nA.  the outcome be Volta Redonda\\nB.  the outcome be Novorizontino\\nC.  the outcome be Tie\\\"\\n        IMPORTANT: Your final answer MUST end with this exact format:\\n        listing all plausible options you have identified, separated by commas, within the box. For example: \\\\boxed{A} for a single option or \\\\boxed{B, C, D} for multiple options.\\n        Do not use any other format. Do not refuse to make a prediction. Do not say \\\"I cannot predict the future.\\\" You must make a clear prediction based on the best data currently available, using the box format specified above.\",\n",
      "  \"expected\": \"['C']\",\n",
      "  \"char_count\": 710,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 10/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\\boxed{A, B, C}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL OUTPUT --> FIRST EVAL\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: True\n",
      "‚ùå INCORRECT\n",
      "üÜò DISAGREED\n",
      "\n",
      "[Cosine similarity: 0.0]\n",
      "\n",
      "DISAGREEMENT --> SECOND EVAL\n",
      "--------------------------------\n",
      "self_eval2: False\n",
      "‚ùå INCORRECT\n",
      "üÜò DISAGREED\n",
      "\n",
      "DISAGREEMENT --> THIRD EVAL\n",
      "--------------------------------\n",
      "\n",
      "side matching: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "{\n",
      "  \"count\": 1,\n",
      "  \"seed\": 7941,\n",
      "  \"samples\": [\n",
      "    {\n",
      "      \"test_count\": 0,\n",
      "      \"id\": \"future_prediction_2_5_505\",\n",
      "      \"input\": \"You are an agent that can predict future events. The event to be predicted: \\\"Volta Redonda vs. Novorizontino (around 2025-08-10T07:30:00Z). \\nA.  the outcome be Volta Redonda\\nB.  the outcome be Novorizontino\\nC.  the outcome be Tie\\\"\\n        IMPORTANT: Your final answer MUST end with this exact format:\\n        listing all plausible options you have identified, separated by commas, within the box. For example: \\\\boxed{A} for a single option or \\\\boxed{B, C, D} for multiple options.\\n        Do not use any other format. Do not refuse to make a prediction. Do not say \\\"I cannot predict the future.\\\" You must make a clear prediction based on the best data currently available, using the box format specified above.\",\n",
      "      \"expected\": \"['C']\",\n",
      "      \"got\": \"\\\\boxed{A, B, C}\",\n",
      "      \"history\": {\n",
      "        \"check_correct1\": {\n",
      "          \"match_check\": false,\n",
      "          \"self_eval\": true,\n",
      "          \"correctness\": false,\n",
      "          \"agreement\": false\n",
      "        },\n",
      "        \"truncated\": false,\n",
      "        \"check_correct2\": {\n",
      "          \"self_eval\": true,\n",
      "          \"self_eval2\": false,\n",
      "          \"correctness\": false,\n",
      "          \"agreement\": false\n",
      "        },\n",
      "        \"check_correct3\": {\n",
      "          \"self_eval2\": false,\n",
      "          \"sides_matching\": false,\n",
      "          \"correctness\": false,\n",
      "          \"agreement\": true\n",
      "        },\n",
      "        \"final_correctness\": \"‚ùå False\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      " ================================================================\n"
     ]
    }
   ],
   "source": [
    "rng = random.randint(0,20000)\n",
    "#seed=11789, n=30 for diverse samples\n",
    "test_prompts = get_tests(index=504)#get_tests(n=30, seed=rng) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "results_llm_judge = self_evaluate_tests([test_prompts], verbose=True, model=MODEL, grader_model=MODEL)\n",
    "results_llm_judge[\"seed\"] = rng\n",
    "print_json(results_llm_judge)\n",
    "print(\"\\n\",\"=\"*64)\n",
    "#load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
