{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "all_tests = json.load(open(\"parsed_dev_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_test(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf'), seed=None):\n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if seed is not None: random.seed(seed)\n",
    "    \n",
    "    if n == 0:\n",
    "        return filtered_tests[start:end]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 440\n",
      "['First',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'many',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'the',\n",
      " 'original',\n",
      " 'bolt',\n",
      " 'of',\n",
      " 'fabric',\n",
      " 'was:',\n",
      " '16',\n",
      " 'feet',\n",
      " '*',\n",
      " '12',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<16*12=192>>192',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Then',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'much',\n",
      " 'fabric',\n",
      " 'Ann',\n",
      " 'took',\n",
      " 'for',\n",
      " 'the',\n",
      " 'living',\n",
      " 'room',\n",
      " 'curtains:',\n",
      " '4',\n",
      " 'feet',\n",
      " '*',\n",
      " '6',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<4*6=24>>24',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Then',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'much',\n",
      " 'fabric',\n",
      " 'Ann',\n",
      " 'took',\n",
      " 'for',\n",
      " 'the',\n",
      " 'bathroom',\n",
      " 'curtains:',\n",
      " '2',\n",
      " 'feet',\n",
      " '*',\n",
      " '4',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<2*4=8>>8',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Finally,',\n",
      " 'subtract',\n",
      " 'the',\n",
      " 'square',\n",
      " 'footage',\n",
      " 'of',\n",
      " 'both',\n",
      " 'sets',\n",
      " 'of',\n",
      " 'curtains',\n",
      " 'from',\n",
      " 'the',\n",
      " 'total',\n",
      " 'square',\n",
      " 'footage:',\n",
      " '192',\n",
      " '-',\n",
      " '24',\n",
      " '-',\n",
      " '8',\n",
      " '=',\n",
      " '<<192-24-8=160>>160',\n",
      " 'square',\n",
      " 'feet',\n",
      " '####',\n",
      " '160']\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=1, upper_char=300) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output, exp):\n",
    "    exp = exp.lower().strip('.')\n",
    "    out = output.lower().strip('.')\n",
    "    \n",
    "    #rare case when exp is actually yes/now and model output is true/false\n",
    "    if exp == \"yes\" and out == \"true\": return \"yes\"\n",
    "    if exp == \"no\" and out == \"false\": return \"no\"\n",
    "    \n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output, exp)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c023deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_matches(toCount, toMatch):\n",
    "    counter = Counter(toCount)\n",
    "    match_counts = {word: counter.get(word, 0) for word in toMatch}\n",
    "    total_matches = sum(match_counts.values())\n",
    "    output_len = len(toCount)\n",
    "    print(f\"{total_matches}/{output_len} : {(total_matches / output_len) * 100 if output_len != 0 else 0}%\")\n",
    "    print('match counts:', match_counts)\n",
    "    return total_matches, output_len\n",
    "\n",
    "def get_cosine(expected_counter, output_counter):\n",
    "    dot_product = sum(expected_counter[word] * output_counter.get(word, 0) for word in expected_counter)\n",
    "    \n",
    "    print(f\"Dot product: {dot_product}\")\n",
    "    \n",
    "    exp_mag = math.sqrt(sum(v**2 for v in expected_counter.values()))\n",
    "    out_mag = math.sqrt(sum(v**2 for v in output_counter.values()))\n",
    "    \n",
    "    if exp_mag > 0 and out_mag > 0:\n",
    "        cosine_sim = dot_product / (exp_mag * out_mag)\n",
    "        print(f\"Cosine similarity: {cosine_sim}\")\n",
    "        \n",
    "    return cosine_sim\n",
    "\n",
    "def super_match(test, output):\n",
    "    expected = test[\"expected\"].lower().split()\n",
    "    output = output.lower().split()\n",
    "    \n",
    "    expected_counter = Counter(expected)\n",
    "    output_counter = Counter(output)\n",
    "    \n",
    "    #not very helpful in the long run...\n",
    "    get_cosine(expected_counter, output_counter)\n",
    "    \n",
    "    exp_matches, out_len = create_matches(output, expected)\n",
    "    out_matches, exp_len = create_matches(expected, output)\n",
    "    \n",
    "    start_matches = False\n",
    "    end_matches = False\n",
    "    if expected[0] in output[0]: start_matches = True\n",
    "    if expected[exp_len-1] in output[out_len-1]: end_matches = True\n",
    "    print(f\"start {start_matches} end {end_matches}\")\n",
    "    #return match_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    \n",
    "    for t in tests:\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_test(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got, t[\"expected\"])\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        super_match(t, got)\n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        seperator('\\nEVALUATION')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        \n",
    "        if not agreement:\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            \n",
    "            if not not_truncated and correctness:\n",
    "                correctness = False\n",
    "                print(\"‚ùå INCORRECT | BOTH EVALS RETURNED TRUE BUT OUTPUT WAS TRUNCATED\")\n",
    "\n",
    "        final_answers.append(correctness)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return final_answers\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 1000\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"math_3_5_605\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Three clever monkeys divide a pile of bananas. The first monkey takes some bananas from the pile, keeps three-fourths of them, and divides the rest equally between the other two. The second monkey takes some bananas from the pile, keeps one-fourth of them, and divides the rest equally between the other two. The third monkey takes the remaining bananas from the pile, keeps one-twelfth of them, and divides the rest equally between the other two. Given that each monkey receives a whole number of bananas whenever the bananas are divided, and the numbers of bananas the first, second, and third monkeys have at the end of the process are in the ratio $3: 2: 1,$ what is the least possible total for the number of bananas?\",\n",
      "  \"expected\": \"408\",\n",
      "  \"char_count\": 722,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 367/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Let the total number of bananas be $ N $.\n",
       "\n",
       "Let the number of bananas taken by the first monkey be $ x $, the second monkey be $ y $, and the third monkey be $ z $, with $ x + y + z = N $.\n",
       "\n",
       "From the problem:\n",
       "\n",
       "- First monkey keeps $ \\frac{3}{4}x $, gives $ \\frac{1}{4}x $ to the other two monkeys (so each gets $ \\frac{1}{8}x $).\n",
       "- Second monkey keeps $ \\frac{1}{4}y $, gives $ \\frac{3}{4}y $ to the other two monkeys (so each gets $ \\frac{3}{8}y $).\n",
       "- Third monkey keeps $ \\frac{1}{12}z $, gives $ \\frac{11}{12}z $ to the other two monkeys (so each gets $ \\frac{11}{24}z $).\n",
       "\n",
       "Let the final number of bananas each monkey has be:\n",
       "\n",
       "- First monkey: $ \\frac{3}{4}x + \\frac{1}{8}y + \\frac{11}{24}z $\n",
       "- Second monkey: $ \\frac{1}{4}x + \\frac{3}{8}y + \\frac{11}{24}z $\n",
       "- Third monkey: $ \\frac{1}{12}z + \\frac{1}{8}x + \\frac{3}{8}y $\n",
       "\n",
       "Given the ratio of their final counts is $ 3:2:1 $, and all divisions must result in whole numbers.\n",
       "\n",
       "After solving the system of equations and ensuring all divisions are integers, the least possible total number of bananas is:\n",
       "\n",
       "**144**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 0\n",
      "Cosine similarity: 0.0\n",
      "0/198 : 0.0%\n",
      "match counts: {'408': 0}\n",
      "0/1 : 0.0%\n",
      "match counts: {'let': 0, 'the': 0, 'total': 0, 'number': 0, 'of': 0, 'bananas': 0, 'be': 0, '$': 0, 'n': 0, '$.': 0, 'taken': 0, 'by': 0, 'first': 0, 'monkey': 0, 'x': 0, '$,': 0, 'second': 0, 'y': 0, 'and': 0, 'third': 0, 'z': 0, 'with': 0, '+': 0, '=': 0, 'from': 0, 'problem:': 0, '-': 0, 'keeps': 0, '\\\\frac{3}{4}x': 0, 'gives': 0, '\\\\frac{1}{4}x': 0, 'to': 0, 'other': 0, 'two': 0, 'monkeys': 0, '(so': 0, 'each': 0, 'gets': 0, '\\\\frac{1}{8}x': 0, '$).': 0, '\\\\frac{1}{4}y': 0, '\\\\frac{3}{4}y': 0, '\\\\frac{3}{8}y': 0, '\\\\frac{1}{12}z': 0, '\\\\frac{11}{12}z': 0, '\\\\frac{11}{24}z': 0, 'final': 0, 'has': 0, 'be:': 0, 'monkey:': 0, '\\\\frac{1}{8}y': 0, 'given': 0, 'ratio': 0, 'their': 0, 'counts': 0, 'is': 0, '3:2:1': 0, 'all': 0, 'divisions': 0, 'must': 0, 'result': 0, 'in': 0, 'whole': 0, 'numbers.': 0, 'after': 0, 'solving': 0, 'system': 0, 'equations': 0, 'ensuring': 0, 'are': 0, 'integers,': 0, 'least': 0, 'possible': 0, 'is:': 0, '**144**': 0}\n",
      "start False end False\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"common_sense_1_333_433\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"Are Jane and First for Women both women's magazines?\",\n",
      "  \"expected\": \"yes\",\n",
      "  \"char_count\": 52,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 3/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "true"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 0\n",
      "Cosine similarity: 0.0\n",
      "0/1 : 0.0%\n",
      "match counts: {'yes': 0}\n",
      "0/1 : 0.0%\n",
      "match counts: {'true': 0}\n",
      "start False end False\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: True\n",
      "self_eval: True\n",
      "‚úÖ CORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"coding_0_33_33\",\n",
      "  \"type\": \"coding\",\n",
      "  \"prompt\": \"Analyze the salary distribution within the department with code 'EMPXX'. Generate random salaries for each employee and create a histogram. - For the department of interest, randomly generate as many salaries as its number of employees. - Make sure that the salary is within SALARY_RANGE. - The histogram title should be 'Salary Distribution in EMPXX Department' - The x-label should be set to 'Salary' - The y-label should be set to 'Number of Employees'\\nThe function should output with:\\n    matplotlib.axes._axes.Axes: Axes object representing the histogram.\\nYou should write self-contained code starting with:\\n```\\nimport random\\nimport matplotlib.pyplot as plt\\n# Constants\\nSALARY_RANGE = (20000, 100000)\\ndef task_func(dict1):\\n```\",\n",
      "  \"expected\": \"    emp_salaries = []\\n\\n    for prefix, num_employees in dict1.items():\\n        if not prefix.startswith('EMPXX'):\\n            continue\\n\\n        for _ in range(num_employees):\\n            salary = random.randint(*SALARY_RANGE)\\n            emp_salaries.append(salary)\\n\\n    plt.hist(emp_salaries, bins=10, alpha=0.5)\\n    plt.title('Salary Distribution in EMPXX Department')\\n    plt.xlabel('Salary')\\n    plt.ylabel('Number of Employees')\\n    return plt.gca()\",\n",
      "  \"char_count\": 731,\n",
      "  \"exp_word_count\": 34\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 137/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "import random\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Constants\n",
       "SALARY_RANGE = (20000, 100000)\n",
       "\n",
       "def task_func(dict1):\n",
       "    dept = dict1.get('EMPXX', {})\n",
       "    num_employees = len(dept)\n",
       "    salaries = [random.randint(*SALARY_RANGE) for _ in range(num_employees)]\n",
       "    \n",
       "    fig, ax = plt.subplots()\n",
       "    ax.hist(salaries, bins=10, edgecolor='black')\n",
       "    ax.set_title('Salary Distribution in EMPXX Department')\n",
       "    ax.set_xlabel('Salary')\n",
       "    ax.set_ylabel('Number of Employees')\n",
       "    \n",
       "    return ax"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 27\n",
      "Cosine similarity: 0.47970161180012355\n",
      "17/46 : 36.95652173913043%\n",
      "match counts: {'emp_salaries': 0, '=': 5, '[]': 0, 'for': 1, 'prefix,': 0, 'num_employees': 1, 'in': 2, 'dict1.items():': 0, 'if': 0, 'not': 0, \"prefix.startswith('empxx'):\": 0, 'continue': 0, '_': 1, 'range(num_employees):': 0, 'salary': 0, 'random.randint(*salary_range)': 0, 'emp_salaries.append(salary)': 0, 'plt.hist(emp_salaries,': 0, 'bins=10,': 1, 'alpha=0.5)': 0, \"plt.title('salary\": 0, 'distribution': 1, 'empxx': 1, \"department')\": 1, \"plt.xlabel('salary')\": 0, \"plt.ylabel('number\": 0, 'of': 1, \"employees')\": 1, 'return': 1, 'plt.gca()': 0}\n",
      "16/34 : 47.05882352941176%\n",
      "match counts: {'import': 0, 'random': 0, 'matplotlib.pyplot': 0, 'as': 0, 'plt': 0, '#': 0, 'constants': 0, 'salary_range': 0, '=': 2, '(20000,': 0, '100000)': 0, 'def': 0, 'task_func(dict1):': 0, 'dept': 0, \"dict1.get('empxx',\": 0, '{})': 0, 'num_employees': 1, 'len(dept)': 0, 'salaries': 0, '[random.randint(*salary_range)': 0, 'for': 2, '_': 1, 'in': 3, 'range(num_employees)]': 0, 'fig,': 0, 'ax': 0, 'plt.subplots()': 0, 'ax.hist(salaries,': 0, 'bins=10,': 1, \"edgecolor='black')\": 0, \"ax.set_title('salary\": 0, 'distribution': 1, 'empxx': 1, \"department')\": 1, \"ax.set_xlabel('salary')\": 0, \"ax.set_ylabel('number\": 0, 'of': 1, \"employees')\": 1, 'return': 1}\n",
      "start False end False\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"coding_0_31_31\",\n",
      "  \"type\": \"coding\",\n",
      "  \"prompt\": \"Create a profit report for a list of products based on a specific product dictionary that includes the quantity, price, and profit of each product. Additionally, calculate the average price and profit for all considered products, and plot a bar chart of the profit for each product.\\nThe function should output with:\\n    tuple: A tuple containing:\\n    DataFrame: A pandas DataFrame with columns\\n    ['Product', 'Quantity', 'Price', 'Profit', 'Average Price', 'Average Profit'].\\n    Axes: A matplotlib Axes object representing the plotted bar chart of profit for each product\\n    (None if no products).\\nYou should write self-contained code starting with:\\n```\\nimport pandas as pd\\nimport numpy as np\\ndef task_func(product_dict, product_keys):\\n```\",\n",
      "  \"expected\": \"    columns = ['Product', 'Quantity', 'Price', 'Profit']\\n    data = []\\n\\n    for key in product_keys:\\n        quantity, price = product_dict[key]\\n        profit = quantity * price\\n        data.append([key, quantity, price, profit])\\n\\n    df = pd.DataFrame(data, columns=columns)\\n\\n    if not df.empty:\\n        # Calculate average price and average profit using numpy\\n        avg_price = np.mean(df['Price'])\\n        avg_profit = np.mean(df['Profit'])\\n\\n        # Add average price and average profit as new columns to the dataframe\\n        df['Average Price'] = avg_price\\n        df['Average Profit'] = avg_profit\\n\\n        ax = df.plot(x='Product', y='Profit', kind='bar', legend=False, title=\\\"Profit for each product\\\")\\n        ax.set_ylabel(\\\"Profit\\\")\\n    else:\\n        ax = None\\n\\n    return df, ax\",\n",
      "  \"char_count\": 742,\n",
      "  \"exp_word_count\": 87\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 139/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "def task_func(product_dict, product_keys):\n",
       "    if not product_keys:\n",
       "        return (None, None)\n",
       "    \n",
       "    df = pd.DataFrame(product_dict, index=product_keys).reset_index()\n",
       "    df.columns = ['Product', 'Quantity', 'Price', 'Profit']\n",
       "    avg_price = df['Price'].mean()\n",
       "    avg_profit = df['Profit'].mean()\n",
       "    df['Average Price'] = avg_price\n",
       "    df['Average Profit'] = avg_profit\n",
       "    \n",
       "    fig, ax = plt.subplots()\n",
       "    ax.bar(df['Product'], df['Profit'])\n",
       "    \n",
       "    return (df, ax)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 108\n",
      "Cosine similarity: 0.6354700506655651\n",
      "30/54 : 55.55555555555556%\n",
      "match counts: {'columns': 0, '=': 7, \"['product',\": 1, \"'quantity',\": 1, \"'price',\": 1, \"'profit']\": 1, 'data': 0, '[]': 0, 'for': 0, 'key': 0, 'in': 0, 'product_keys:': 1, 'quantity,': 0, 'price': 0, 'product_dict[key]': 0, 'profit': 0, 'quantity': 0, '*': 0, 'data.append([key,': 0, 'price,': 0, 'profit])': 0, 'df': 1, 'pd.dataframe(data,': 0, 'columns=columns)': 0, 'if': 1, 'not': 1, 'df.empty:': 0, '#': 0, 'calculate': 0, 'average': 0, 'and': 0, 'using': 0, 'numpy': 1, 'avg_price': 2, \"np.mean(df['price'])\": 0, 'avg_profit': 2, \"np.mean(df['profit'])\": 0, 'add': 0, 'as': 3, 'new': 0, 'to': 0, 'the': 0, 'dataframe': 0, \"df['average\": 2, \"price']\": 1, \"profit']\": 1, 'ax': 1, \"df.plot(x='product',\": 0, \"y='profit',\": 0, \"kind='bar',\": 0, 'legend=false,': 0, 'title=\"profit': 0, 'each': 0, 'product\")': 0, 'ax.set_ylabel(\"profit\")': 0, 'else:': 0, 'none': 0, 'return': 2, 'df,': 0}\n",
      "33/87 : 37.93103448275862%\n",
      "match counts: {'import': 0, 'pandas': 0, 'as': 1, 'pd': 0, 'numpy': 1, 'np': 0, 'matplotlib.pyplot': 0, 'plt': 0, 'def': 0, 'task_func(product_dict,': 0, 'product_keys):': 0, 'if': 1, 'not': 1, 'product_keys:': 1, 'return': 1, '(none,': 0, 'none)': 0, 'df': 1, '=': 11, 'pd.dataframe(product_dict,': 0, 'index=product_keys).reset_index()': 0, 'df.columns': 0, \"['product',\": 1, \"'quantity',\": 1, \"'price',\": 1, \"'profit']\": 1, 'avg_price': 2, \"df['price'].mean()\": 0, 'avg_profit': 2, \"df['profit'].mean()\": 0, \"df['average\": 2, \"price']\": 1, \"profit']\": 1, 'fig,': 0, 'ax': 3, 'plt.subplots()': 0, \"ax.bar(df['product'],\": 0, \"df['profit'])\": 0, '(df,': 0, 'ax)': 0}\n",
      "start False end True\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"coding_0_96_96\",\n",
      "  \"type\": \"coding\",\n",
      "  \"prompt\": \"Count the occurrence of values with the keys \\\"x,\\\" \\\"y\\\" and \\\"z\\\" from a list of dictionaries \\\"d.\\\"\\nThe function should output with:\\n    dict: A dictionary with keys as 'x', 'y', and 'z' and values as Counter objects.\\nYou should write self-contained code starting with:\\n```\\nimport pandas as pd\\nfrom collections import Counter\\ndef task_func(d):\\n```\",\n",
      "  \"expected\": \"    df = pd.DataFrame(d)\\n    counts = {}\\n\\n    for key in ['x', 'y', 'z']:\\n        if key in df.columns:\\n            counts[key] = Counter(df[key].dropna().tolist())\\n        else:\\n            counts[key] = Counter()\\n\\n    return counts\",\n",
      "  \"char_count\": 342,\n",
      "  \"exp_word_count\": 25\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 80/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from collections import Counter\n",
       "def task_func(d):\n",
       "    counts = {k: Counter() for k in ['x', 'y', 'z']}\n",
       "    for item in d:\n",
       "        for key in ['x', 'y', 'z']:\n",
       "            if key in item:\n",
       "                counts[key][item[key]] += 1\n",
       "    return counts\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 31\n",
      "Cosine similarity: 0.5563279654601043\n",
      "20/41 : 48.78048780487805%\n",
      "match counts: {'df': 0, '=': 1, 'pd.dataframe(d)': 0, 'counts': 2, '{}': 0, 'for': 3, 'key': 2, 'in': 4, \"['x',\": 2, \"'y',\": 2, \"'z']:\": 1, 'if': 1, 'df.columns:': 0, 'counts[key]': 0, 'counter(df[key].dropna().tolist())': 0, 'else:': 0, 'counter()': 1, 'return': 1}\n",
      "17/25 : 68.0%\n",
      "match counts: {'```python': 0, 'import': 0, 'pandas': 0, 'as': 0, 'pd': 0, 'from': 0, 'collections': 0, 'counter': 0, 'def': 0, 'task_func(d):': 0, 'counts': 2, '=': 4, '{k:': 0, 'counter()': 1, 'for': 1, 'k': 0, 'in': 2, \"['x',\": 1, \"'y',\": 1, \"'z']}\": 0, 'item': 0, 'd:': 0, 'key': 2, \"'z']:\": 1, 'if': 1, 'item:': 0, 'counts[key][item[key]]': 0, '+=': 0, '1': 0, 'return': 1, '```': 0}\n",
      "start False end False\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "[False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "test_prompts = get_tests(n=5) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\n",
    "print(\"\\n\",\"=\"*64)\n",
    "print(results_llm_judge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
