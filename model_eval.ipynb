{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "all_tests = json.load(open(\"parsed_dev_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf')):\n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "\n",
    "    if n == 0:\n",
    "        return filtered_tests[start:end]\n",
    "    else:\n",
    "        sample_size = min(n, len(filtered_tests))\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'char_count': 29,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': False,\n",
      "  'id': 'common_sense_1_399_499',\n",
      "  'prompt': 'Is a pound sterling valuable?',\n",
      "  'type': 'common_sense'},\n",
      " {'char_count': 187,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': '3',\n",
      "  'id': 'math_3_191_791',\n",
      "  'prompt': 'Ten treeks weigh as much as three squigs and one goolee. Two '\n",
      "            'treeks and one goolee are equal in weight to one squig. The '\n",
      "            'combined weight of how many treeks equals the weight of one '\n",
      "            'squig?',\n",
      "  'type': 'math'},\n",
      " {'char_count': 58,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': True,\n",
      "  'id': 'common_sense_1_313_413',\n",
      "  'prompt': 'Is Alan Alda old enough to have fought in the Vietnam War?',\n",
      "  'type': 'common_sense'},\n",
      " {'char_count': 39,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': '4',\n",
      "  'id': 'math_3_291_891',\n",
      "  'prompt': 'If $2^8=4^x$, what is the value of $x$?',\n",
      "  'type': 'math'},\n",
      " {'char_count': 333,\n",
      "  'exp_word_count': 9,\n",
      "  'expected': 'No, humans have never flown close to the sun',\n",
      "  'id': 'common_sense_1_172_272',\n",
      "  'prompt': 'Have humans flown close to the sun?\\n'\n",
      "            'What is the best answer for the question among these?\\n'\n",
      "            '0) No, humans have remained millions of miles away from the sun\\n'\n",
      "            '1) No, humans have never flown close to the sun\\n'\n",
      "            '2) Yes, Icarus flew too close to the sun\\n'\n",
      "            '3) No, humans have never flown close to the sun\\n'\n",
      "            '4) Yes, humans have flown close to the sun',\n",
      "  'type': 'common_sense'}]\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=5, upper_char=400) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    rows = []\n",
    "    count = 0\n",
    "    for t in tests:\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print_test(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        display(Markdown(f\"OUTPUT: \\n{got}\"))\n",
    "        print('raw: ', got)\n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        \"\"\" is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"id\": t.get(\"id\", \"<unnamed>\"),\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "        print(json.dumps(row, indent=2, ensure_ascii=False))\n",
    "        if verbose:\n",
    "            mark = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{mark} {row['id']}: expected={row['expected']!r}, got={row['got']!r} (HTTP {row['status']})\")\n",
    "            if row[\"error\"]:\n",
    "                print(\"   error:\", row[\"error\"]) \"\"\"\n",
    "\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    matches = re.findall(exp, output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if len(num_matches) > 0:\n",
    "        print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    print('NO MATCH FOUND')\n",
    "    return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" if exp.lower() in output.lower():\n",
    "        print('FOUND MATCH SOMEWHERE')\n",
    "        if len(exp) < 3:\n",
    "            \n",
    "        else:\n",
    "            #exp is too large for basic match\n",
    "            print('EXPECTED TOO LARGE')\n",
    "    \n",
    "    #if we find zero matches at all just return false\n",
    "    print('NO MATCH')\n",
    "    return False \"\"\"\n",
    "\n",
    "\"\"\" if test['type'] == 'math':\n",
    "    num_matches = re.findall(r'-?\\d+(\\.\\d+)?', output)\n",
    "word_matches = re.findall(r'[a-zA-Z]+', output)\n",
    "if matches:\n",
    "    return True\n",
    "return False \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"planning_4_77_977\",\n",
      "  \"type\": \"planning\",\n",
      "  \"prompt\": \"I am playing with a set of objects. Here are the actions I can do\\n\\n   Attack object\\n   Feast object from another object\\n   Succumb object\\n   Overcome object from another object\\n\\nI have the following restrictions on my actions:\\n    To perform Attack action, the following facts need to be true: Province object, Planet object, Harmony.\\n    Once Attack action is performed the following facts will be true: Pain object.\\n    Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.\\n    To perform Succumb action, the following facts need to be true: Pain object.\\n    Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.    \\n    Once Succumb action is performed the following facts will be false: Pain object.\\n    To perform Overcome action, the following needs to be true: Province other object, Pain object.\\n    Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.\\n    Once Overcome action is performed the following will be false: Province other object, Pain object.\\n    To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.\\n    Once Feast action is performed the following will be true: Pain object, Province other object.\\n    Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.\\n\\n[STATEMENT]\\nAs initial conditions I have that, object b craves object c, harmony, planet object a, planet object c, planet object d, province object a, province object b and province object d.\\nMy goal is to have that object a craves object b, object b craves object d and object c craves object a.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nfeast object b from object c\\novercome object b from object d\\nattack object a\\novercome object a from object b\\nattack object c\\novercome object c from object a\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, object b craves object a, harmony, planet object a, planet object c, planet object d, province object b, province object c and province object d.\\nMy goal is to have that object c craves object b and object d craves object a.\\n\\nMy plan is as follows:\\n\\n[PLAN]\",\n",
      "  \"expected\": \"(feast b a)\\n(succumb b)\\n(attack c)\\n(overcome c b)\\n(attack d)\\n(overcome d a)\\n\",\n",
      "  \"char_count\": 2271,\n",
      "  \"exp_word_count\": 15\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "feast object a from object b  \n",
       "overcome object a from object d  \n",
       "attack object b  \n",
       "overcome object b from object a  \n",
       "attack object d  \n",
       "overcome object d from object a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  feast object a from object b  \n",
      "overcome object a from object d  \n",
      "attack object b  \n",
      "overcome object b from object a  \n",
      "attack object d  \n",
      "overcome object d from object a\n",
      "{\n",
      "  \"id\": \"math_3_108_708\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Leo's assignment was divided into three parts. He finished the first part of his assignment in 25 minutes. It took him twice as long to finish the second part. If he was able to finish his assignment in 2 hours, how many minutes did Leo finish the third part of the assignment?\",\n",
      "  \"expected\": \"It took Leo 25 x 2 = <<25*2=50>>50 minutes to finish the second part of the assignment.\\nLeo finished the first and second parts of the assignment in 25 + 50 = <<25+50=75>>75 minutes.\\nHe finished the entire assignment in 60 x 2 = <<60*2=120>>120 minutes.\\nTherefore, it took Leo 120 - 75 = <<120-75=45>>45 minutes to finish the third part of the assignment.\\n#### 45\",\n",
      "  \"char_count\": 277,\n",
      "  \"exp_word_count\": 66\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "First part: 25 minutes  \n",
       "Second part: 2 × 25 = 50 minutes  \n",
       "Total time: 2 hours = 120 minutes  \n",
       "Third part: 120 - 25 - 50 = 45 minutes  \n",
       "\n",
       "45"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  First part: 25 minutes  \n",
      "Second part: 2 × 25 = 50 minutes  \n",
      "Total time: 2 hours = 120 minutes  \n",
      "Third part: 120 - 25 - 50 = 45 minutes  \n",
      "\n",
      "45\n",
      "{\n",
      "  \"id\": \"math_3_251_851\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Let $z$ be a complex number such that $z^5 = 1$ and $z \\\\neq 1.$  Compute\\n\\\\[z + \\\\frac{1}{z} + z^2 + \\\\frac{1}{z^2}.\\\\]\",\n",
      "  \"expected\": \"-1\",\n",
      "  \"char_count\": 115,\n",
      "  \"exp_word_count\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "$-1$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  $-1$\n"
     ]
    }
   ],
   "source": [
    "test_prompts = get_tests(n=3) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
