{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbfe985",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Main chat completion function\n",
    "\n",
    "---\n",
    "\n",
    "Same function from tutorial, with some additions like extra hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128,\n",
    "                                top_p: int = None,\n",
    "                                top_k: int = None,\n",
    "                                stop: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"stop\": stop\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f403e",
   "metadata": {},
   "source": [
    "---\n",
    "# TESTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c16fd",
   "metadata": {},
   "source": [
    "Get all the tests, load/save, define possible types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "def load_save_json(path_in=\"parsed_dev_data.json\", path_out=None, data_in=None, clear=False):\n",
    "    data = json.load(open(path_in, \"r\", encoding=\"utf-8\")) if not clear else []\n",
    "    if path_out is not None:\n",
    "        data.append(data_in)\n",
    "        with open(path_out, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "            \n",
    "    return data\n",
    "            \n",
    "all_tests = load_save_json()\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421dedf",
   "metadata": {},
   "source": [
    "Generic get test function, mostly used for getting random tests of certain type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_json(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf'), seed=None, index=None):\n",
    "    if index is not None: return all_tests[index]\n",
    "    \n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    #print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if seed is not None: random.seed(seed)\n",
    "    \n",
    "    if n == 0:\n",
    "        return [filtered_tests[start:end]]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        #print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages) \"\"\"\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60a80b",
   "metadata": {},
   "source": [
    "### Handle guessing the prompt domain/type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3400d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcd7a8",
   "metadata": {},
   "source": [
    "Terrible by itself (This qwen model is just not very capable...), might work better with this new implementation to supplement low NB confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c64c8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs really really bad\n",
    "def guess_test_type(input, p1, p2):\n",
    "    sys = \"\"\"\n",
    "    You are a basic decider model.\n",
    "    Your goal is to distinguish between if an input sentence represents a math, common sense, planning, or coding question.\n",
    "    Respond with one word depending on the input sentence type\n",
    "    \"\"\"\n",
    "    #POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "    prompt = \"\"\"\n",
    "    What question type does the following input correspond to?\n",
    "    \n",
    "    INPUT: {input}\n",
    "    ----------------------\n",
    "    Is it a {p1} question?\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    Is it a {p2} question?\n",
    "    \n",
    "    Respond with one word\n",
    "    \"\"\"\n",
    "    return call_model_chat_completions(system=sys, prompt=prompt, temperature=0.3)[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf38a12",
   "metadata": {},
   "source": [
    "Simple NB classifer trained in colab on the dev data. Good test accuracy around 95.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e56e4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nb_classifier.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "with open('nb_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f9d4cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('cse_476_final_project_test_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0da5607f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for test,i in zip(data,range(0,100)):\\n    print(test)\\n    #print(\\'ACTUAL:\\', test[\"domain\"])\\n    result = classify_prompt(test[\"input\"])\\n    print(f\"PREDICTED: {result[\\'predicted_type\\']}\")\\n    print()\\n    print(f\"Probabilities: {result[\\'probabilities\\']}\") '"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_prompt(prompt):\n",
    "    prompt_vec = vectorizer.transform([prompt])\n",
    "    \n",
    "    prediction = classifier.predict(prompt_vec)[0]\n",
    "    probabilities = classifier.predict_proba(prompt_vec)[0]\n",
    "    \n",
    "    print(f\"prediction: {prediction}\")\n",
    "    print(f\"Classes order: {classifier.classes_}\")\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    \n",
    "    result = {\n",
    "        'prompt': prompt,\n",
    "        'predicted_type': prediction,\n",
    "        'probabilities': {class_name: prob for class_name, prob in zip(classifier.classes_, probabilities)}\n",
    "    }\n",
    "    \n",
    "    high_prob_classes = [class_name for class_name, prob in zip(classifier.classes_, probabilities) if prob >= 0.40]\n",
    "    \n",
    "    if len(high_prob_classes) == 2:\n",
    "        print(f\"TWO HIGH PROB: {high_prob_classes[0]} {high_prob_classes[1]}\")\n",
    "        return guess_test_type(prompt, high_prob_classes[0], high_prob_classes[1])\n",
    "    \n",
    "    return result['predicted_type']\n",
    "\n",
    "\"\"\" for test,i in zip(data,range(0,100)):\n",
    "    print(test)\n",
    "    #print('ACTUAL:', test[\"domain\"])\n",
    "    result = classify_prompt(test[\"input\"])\n",
    "    print(f\"PREDICTED: {result['predicted_type']}\")\n",
    "    print()\n",
    "    print(f\"Probabilities: {result['probabilities']}\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb855256",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# First Final Project Implementation\n",
    "\n",
    "---\n",
    "\n",
    "So I didn't realize we don't get expected answers on the final test data, my first implementation of the final project was based off the tutorial notebooks using expected answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba6010",
   "metadata": {},
   "source": [
    "### Self Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8876e3",
   "metadata": {},
   "source": [
    "### Eval util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output, exp):\n",
    "    exp = str(exp)\n",
    "    exp = exp.lower().strip('.')\n",
    "    out = output.lower().strip('.')\n",
    "    \n",
    "    #rare case when exp is actually yes/now and model output is true/false\n",
    "    if exp == \"yes\" and out == \"true\": return \"yes\"\n",
    "    if exp == \"no\" and out == \"false\": return \"no\"\n",
    "    \n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output, exp)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381d948",
   "metadata": {},
   "source": [
    "Bunch of functions used when we have an expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c023deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_matches(toCount, toMatch):\n",
    "    counter = Counter(toCount)\n",
    "    match_counts = {word: counter.get(word, 0) for word in toMatch}\n",
    "    total_matches = sum(match_counts.values())\n",
    "    output_len = len(toCount)\n",
    "    #print(f\"{total_matches}/{output_len} : {(total_matches / output_len) * 100 if output_len != 0 else 0}%\")\n",
    "    #print('match counts:', match_counts)\n",
    "    return total_matches, output_len\n",
    "\n",
    "def get_cosine(expected_counter, output_counter):\n",
    "    dot_product = sum(expected_counter[word] * output_counter.get(word, 0) for word in expected_counter)\n",
    "    \n",
    "    #print(f\"Dot product: {dot_product}\")\n",
    "    \n",
    "    exp_mag = math.sqrt(sum(v**2 for v in expected_counter.values()))\n",
    "    out_mag = math.sqrt(sum(v**2 for v in output_counter.values()))\n",
    "    \n",
    "    cosine_sim = 0\n",
    "    if exp_mag > 0 and out_mag > 0:\n",
    "        cosine_sim = dot_product / (exp_mag * out_mag)\n",
    "        print(f\"\\n[Cosine similarity: {cosine_sim}]\")\n",
    "        \n",
    "    return cosine_sim\n",
    "\n",
    "def get_start_end_matches(expected, output, exp_len, out_len):\n",
    "    start_matches = False\n",
    "    end_matches = False\n",
    "    if expected[0] in output[0]: start_matches = True\n",
    "    if expected[exp_len-1] in output[out_len-1]: end_matches = True\n",
    "    #print('exp', expected)\n",
    "    #print('output', output)\n",
    "    \n",
    "    #print(f\"expected[0] {expected[0]}, output[0] {output[0]}\")\n",
    "    #print(f\"expected[exp_len-1] {expected[exp_len-1]}, output[out_len-1] {output[out_len-1]}\")\n",
    "    #print(f\"START {start_matches} END {end_matches}\")\n",
    "    \n",
    "    return start_matches, end_matches\n",
    "    \n",
    "def super_match(test, output):\n",
    "    expected = str(test[\"expected\"]).replace('$', '').lower().split()\n",
    "    output = output.replace('$', '').lower().split()\n",
    "    \n",
    "    expected_counter = Counter(expected)\n",
    "    output_counter = Counter(output)\n",
    "    \n",
    "    #not very helpful in the long run...\n",
    "    get_cosine(expected_counter, output_counter)\n",
    "    \n",
    "    exp_matches, out_len = create_matches(output, expected)\n",
    "    out_matches, exp_len = create_matches(expected, output)\n",
    "    \n",
    "    return get_start_end_matches(expected, output, exp_len, out_len)\n",
    "    #return match_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790f41d",
   "metadata": {},
   "source": [
    "### Main first implementation agent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    test_samples = {\n",
    "        \"count\": len(tests),\n",
    "        \"seed\": None,\n",
    "        \"samples\": None\n",
    "    }\n",
    "    \n",
    "    for t in tests:\n",
    "        sample = {\n",
    "            \"test_count\": count,\n",
    "            \"id\": t[\"id\"],\n",
    "            \"input\": t['prompt'],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": None,\n",
    "            \"history\": {\n",
    "                \"check_correct1\": {\n",
    "                    \"match_check\": None,\n",
    "                    \"self_eval\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"no_output\": False,\n",
    "                \"truncated\": False,\n",
    "                \"check_correct2\": {\n",
    "                    \"self_eval\": None,\n",
    "                    \"self_eval2\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"check_correct3\": {\n",
    "                    \"self_eval2\": None,\n",
    "                    \"sides_matching\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"final_correctness\": None\n",
    "            }\n",
    "        }\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_json(t)\n",
    "        #handle_test(t)\n",
    "        \n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        sample[\"got\"]=got\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got, t[\"expected\"])\n",
    "        \n",
    "        has_output  = True if got != \"\" else False\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        \n",
    "        \n",
    "        got = handle_test(t, got)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        \n",
    "        print_json(got)\n",
    "        #print(got)\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        if not not_truncated:\n",
    "            #final_answers.append(False)\n",
    "            sample[\"history\"]['truncated'] = True\n",
    "            print(\"‚ùå MAX TOKENS REACHED, OUTPUT TRUNCATED, SKIPPING TESTCASE ‚ùå\")\n",
    "            continue\n",
    "        elif has_output == False:\n",
    "            sample[\"history\"]['no_output'] = True\n",
    "            print(\"‚ùå NO OUTPUT, PROMPT IS PROBABLY TOO LARGE, SKIPPING TESTCASE ‚ùå\")\n",
    "            continue\n",
    "        \n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        sample[\"history\"][\"check_correct1\"][\"match_check\"] = match_check\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        sample[\"history\"][\"check_correct1\"][\"self_eval\"] = is_correct\n",
    "        \n",
    "        seperator('\\nMODEL OUTPUT --> FIRST EVAL')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        sample[\"history\"][\"check_correct1\"]['correctness'] = correctness\n",
    "        sample[\"history\"][\"check_correct1\"]['agreement'] = agreement\n",
    "        \n",
    "        #starting and ending matches\n",
    "        #CAN BE USED TO VALIDATE SECOND MODEL, OR AS LAST RESORT\n",
    "        start_matches, end_matches = super_match(t, got)\n",
    "        sides_matching = start_matches or end_matches\n",
    "        \n",
    "        if not agreement:\n",
    "            #second model eval\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval\"] = is_correct\n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval2\"] = is_correct2\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            sample[\"history\"][\"check_correct2\"][\"correctness\"] = correctness\n",
    "            sample[\"history\"][\"check_correct2\"][\"agreement\"] = agreement\n",
    "            \n",
    "            \n",
    "            if not agreement:\n",
    "                #second model eval\n",
    "                seperator('\\nDISAGREEMENT --> THIRD EVAL')\n",
    "                print('\\nside matching:', sides_matching)\n",
    "                \n",
    "                sample[\"history\"][\"check_correct3\"][\"self_eval2\"] = is_correct2\n",
    "                sample[\"history\"][\"check_correct3\"][\"sides_matching\"] = sides_matching\n",
    "                correctness, agreement = check_correct(sides_matching, is_correct2)\n",
    "                sample[\"history\"][\"check_correct3\"][\"correctness\"] = correctness\n",
    "                sample[\"history\"][\"check_correct3\"][\"agreement\"] = agreement    \n",
    "\n",
    "\n",
    "        sample[\"history\"][\"final_correctness\"] = f\"‚úÖ {correctness}\" if correctness else f\"‚ùå {correctness}\"\n",
    "        final_answers.append(sample)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    test_samples[\"samples\"] = final_answers\n",
    "    return test_samples\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e51dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Second Final Project Implementation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e78bf",
   "metadata": {},
   "source": [
    "## Math section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54501a74",
   "metadata": {},
   "source": [
    "Importing arithmatic solver inference time algorithm from mini lab 5. This inference time agent is based off the one from mini lab 5, but also contains a ton of extra additions like more math functions and extra handling, as well as a custom inference loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a06deada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, textwrap, re, time, ast, operator as op\n",
    "import requests\n",
    "import math\n",
    "# --- PROVIDED: prompts ---\n",
    "SYSTEM_AGENT2 = \"\"\"You are a math tool-using agent.\n",
    "You may do exactly ONE of the following in your reply:\n",
    "1) CALCULATE: <arithmetic expression>\n",
    "   - use only numbers, + - * / **, parentheses, and round(x, ndigits)\n",
    "   - example: CALCULATE: round((3*2.49)*1.07, 2)\n",
    "2) FINAL: <answer>\n",
    "Return ONE line with the directive and value. No other text.\n",
    "\"\"\"\n",
    "SYSTEM_AGENT = \"\"\"You are a math tool-using agent.\n",
    "\n",
    "IMPORTANT: You must respond with EXACTLY ONE LINE in one of these formats:\n",
    "1) CALCULATE: <arithmetic expression>\n",
    "2) FINAL: <numeric answer>\n",
    "\n",
    "Rules:\n",
    "- NO explanations or reasoning\n",
    "- NO LaTeX or markdown\n",
    "- Arithmetic expressions can only use: numbers, +, -, *, /, **, (), round()\n",
    "- Example valid responses:\n",
    "  CALCULATE: (37 + 58) * 2\n",
    "  CALCULATE: round(23.80 * 1.15, 2)\n",
    "  FINAL: 190\n",
    "  FINAL: 27.37\n",
    "\n",
    "Respond with ONLY the directive line, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def make_first_prompt(question: str) -> str:\n",
    "    return f\"\"\"Question: {question}\n",
    "If you need arithmetic to get the answer, reply as:\n",
    "CALCULATE: <expression>\n",
    "Otherwise reply:\n",
    "FINAL: <answer>\"\"\"\n",
    "\n",
    "def make_second_prompt(result: str) -> str:\n",
    "    return f\"\"\"The calculation result is: {result}\n",
    "Now provide the final answer.\n",
    "Reply exactly as: FINAL: <answer>\"\"\"\n",
    "\n",
    "\n",
    "ACTION_RE = re.compile(r\"^\\s*(CALCULATE|FINAL)\\s*:\\s*(.+?)\\s*$\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "\n",
    "def parse_action(text: str):\n",
    "    \"\"\"\n",
    "    Returns (\"CALCULATE\", expr) or (\"FINAL\", answer); raises ValueError on bad format.\n",
    "    \"\"\"\n",
    "    # Take only the first line\n",
    "    first_line = text.strip().split('\\n')[0]\n",
    "    m = ACTION_RE.match(first_line)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unrecognized action format: {text!r}\")\n",
    "    action = m.group(1).upper()\n",
    "    payload = m.group(2).strip()\n",
    "    return action, payload\n",
    "\n",
    "# We provide this function that evaluates arithmetic expressions.\n",
    "ALLOWED_BINOPS = {ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv, ast.Pow: op.pow, ast.Mod: op.mod}\n",
    "ALLOWED_UNOPS  = {ast.UAdd: op.pos, ast.USub: op.neg}\n",
    "\n",
    "def handle_vars(expr: str, question: str):\n",
    "    #SYSTEM_AGENT = f\"\"\n",
    "\n",
    "    new_prompt = f\"\"\"\n",
    "                YOUR GOAL IS TO REPLACE VARIABLES WITH NUMBERS\n",
    "                LETTERS ARE NOT ALLOWED IN THE EXPRESSION.\n",
    "                \n",
    "                A variable(s) was detected in this expression: {expr}\n",
    "                \n",
    "                Here is the original question: {question}\n",
    "                \n",
    "                Replace each variable (letter) with a number (to the best of your ability) and return ONLY the new expression. No explanation. No letters.\n",
    "              \"\"\"\n",
    "    res = call_model_chat_completions(system=SYSTEM_AGENT, prompt=new_prompt)\n",
    "    print(\"handle vars res:\", res[\"text\"])\n",
    "    return ast.parse(res[\"text\"], mode=\"eval\")\n",
    "    \n",
    "    \n",
    "def safe_eval(expr: str, question: str):\n",
    "    \"\"\"\n",
    "    Evaluates a tiny arithmetic language: numbers, + - * / ** % parentheses, round(x, ndigits).\n",
    "    Converts '^' to '**'. Rejects anything else.\n",
    "    \"\"\"\n",
    "    expr = expr.replace(\"^\", \"**\")\n",
    "    expr = expr.replace(\"i\", \"j\")\n",
    "    if len(expr) > 300: #changed from 200 to 300.\n",
    "        raise ValueError(\"Expression too long.\")\n",
    "    node = ast.parse(expr, mode=\"eval\")\n",
    "\n",
    "    count = 0\n",
    "    def ev(n):\n",
    "        if isinstance(n, ast.Expression):  return ev(n.body)\n",
    "        if isinstance(n, ast.Constant) and isinstance(n.value, (int, float, complex)): return n.value\n",
    "        if isinstance(n, ast.UnaryOp) and type(n.op) in ALLOWED_UNOPS:        return ALLOWED_UNOPS[type(n.op)](ev(n.operand))\n",
    "        if isinstance(n, ast.BinOp) and type(n.op) in ALLOWED_BINOPS:         return ALLOWED_BINOPS[type(n.op)](ev(n.left), ev(n.right))\n",
    "        if isinstance(n, ast.Call) and isinstance(n.func, ast.Name) and n.func.id in [\"round\", \"abs\", \"sqrt\", \"ceil\"]:\n",
    "            args = [ev(a) for a in n.args]\n",
    "            if n.func.id == \"round\":\n",
    "                return round(*args)\n",
    "            elif n.func.id == \"abs\":\n",
    "                return abs(*args)\n",
    "            elif n.func.id == \"sqrt\":\n",
    "                return math.sqrt(*args)\n",
    "            elif n.func.id == \"ceil\":\n",
    "                return math.ceil(*args)\n",
    "        if isinstance(n, ast.Tuple):  # allow round(x,2) with comma\n",
    "            return tuple(ev(elt) for elt in n.elts)\n",
    "        \n",
    "        nonlocal count\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            print(f\"Disallowed expression: {ast.dump(n, include_attributes=False)}\")\n",
    "            return ev(handle_vars(expr, question))\n",
    "        raise ValueError(f\"Disallowed expression: {ast.dump(n, include_attributes=False)}\")\n",
    "\n",
    "    return ev(node)\n",
    "\n",
    "\n",
    "\n",
    "def run_agent(question: str, max_tool_uses: int = 2, verbose: bool = True):\n",
    "    # Turn 1\n",
    "    r1 = call_model_chat_completions(system=SYSTEM_AGENT, prompt=make_first_prompt(question))\n",
    "    if not r1[\"ok\"]:\n",
    "        raise RuntimeError(f\"API error: {r1['error']}\")\n",
    "    if verbose: print(\"LLM ‚Üí\", r1[\"text\"])\n",
    "    action, payload = parse_action(r1[\"text\"])\n",
    "\n",
    "    tool_uses = 0\n",
    "    while action == \"CALCULATE\":\n",
    "        if tool_uses >= max_tool_uses:\n",
    "            raise RuntimeError(\"Exceeded tool-use limit.\")\n",
    "        tool_uses += 1\n",
    "\n",
    "        calc_value = safe_eval(payload, question) #pass it question so we can handle variables\n",
    "        if verbose: print(\"CALC =\", calc_value)\n",
    "\n",
    "        # Turn 2 (+)\n",
    "        rN = call_model_chat_completions(system=SYSTEM_AGENT, prompt=make_second_prompt(str(calc_value)))\n",
    "        if not rN[\"ok\"]:\n",
    "            raise RuntimeError(f\"API error: {rN['error']}\")\n",
    "        if verbose: print(\"LLM ‚Üí\", rN[\"text\"])\n",
    "\n",
    "        action, payload = parse_action(rN[\"text\"])\n",
    "\n",
    "    # action must be FINAL here\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc59d54",
   "metadata": {},
   "source": [
    "Implementing the AST parser and math handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae6ef413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "\n",
    "# ---------- Baseline: no-tool runner ----------\n",
    "SYSTEM_DIRECT = \"You are a careful math assistant. Reply with only the final numeric answer‚Äîno explanation.\"\n",
    "\n",
    "NUM_RE = re.compile(r\"[-+]?\\d+(?:\\.\\d+)?(?:[eE][-+]?\\d+)?\")\n",
    "\n",
    "def extract_number(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to normalize model output to a single numeric string.\n",
    "    Falls back to the raw text if no number is found.\n",
    "    \"\"\"\n",
    "    m = NUM_RE.search(text)\n",
    "    num = m.group(0) if m else text.strip()\n",
    "    #print(f\"extracted num: {num}\")\n",
    "    return num\n",
    "\n",
    "def make_direct_final_prompt(result: str) -> str:\n",
    "    return f\"\"\"The calculation result is: {result}\n",
    "            Now provide the final answer, no explanation.\n",
    "            Reply exactly as: FINAL: <answer>\"\"\"\n",
    "\n",
    "def run_direct(question: str, verbose: bool = True) -> str:\n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=question,\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    if not r[\"ok\"]:\n",
    "        raise RuntimeError(f\"API error: {r['error']}\")\n",
    "    if verbose:\n",
    "        print(\"LLM(no-tool) ‚Üí\", r[\"text\"])\n",
    "        \n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=make_direct_final_prompt(r[\"text\"]),\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    print(f\"final direct: {r[\"text\"]}\")\n",
    "    return extract_number(r[\"text\"])\n",
    "\n",
    "# ---------- If you need QUESTIONS / is_correct ----------\n",
    "QUESTIONS = [\n",
    "    (\"What is (37 + 58) * 2?\", \"190\"),\n",
    "    (\"A class has 28 students; 25% are absent. How many are present?\", \"21\"),\n",
    "    (\"Solve 3x + 5 = 26 for x.\", \"7\"),\n",
    "    (\"What is 12% of 240?\", \"28.8\"),\n",
    "    (\"Average of 12, 18, 29, 31?\", \"22.5\"),\n",
    "    (\"3 notebooks cost $2.49 each, plus 7% tax. Total to 2 decimals?\", \"7.99\"),\n",
    "    (\"Convert 3.5 hours to minutes.\", \"210\"),\n",
    "    (\"Perimeter of a rectangle 8 by 11.\", \"38\"),\n",
    "    (\"What is 2.5^3?\", \"15.625\"),\n",
    "    (\"Add a 15% tip to $23.80, round to 2 decimals.\", \"27.37\"),\n",
    "]\n",
    "\n",
    "def is_correct(pred: str, gold: str):\n",
    "    try:\n",
    "        return abs(float(pred) - float(gold)) <= 1e-6\n",
    "    except:\n",
    "        return pred.strip() == gold.strip()\n",
    "\n",
    "# ---------- Evaluation harness ----------\n",
    "def evaluate_side_by_side(questions=QUESTIONS, verbose=False):\n",
    "    rows = []\n",
    "    direct_correct = 0\n",
    "    tool_correct   = 0\n",
    "\n",
    "    for i, (q, gold, ttype) in enumerate(questions, 1):\n",
    "        gold = re.findall(r\"\\d+\", gold)[-1] if len(gold) > 5 else gold\n",
    "        \n",
    "        if verbose: print(f\"\\nQ{i}: {q}\")\n",
    "\n",
    "        # No-tool\n",
    "        pred_direct = run_direct(q, verbose=verbose)\n",
    "        ok_direct   = is_correct(pred_direct, gold)\n",
    "        direct_correct += int(ok_direct)\n",
    "\n",
    "        # With tool\n",
    "        pred_tool = run_agent(q, verbose=verbose)  # uses your agent loop\n",
    "        ok_tool   = is_correct(pred_tool, gold)\n",
    "        tool_correct += int(ok_tool)\n",
    "\n",
    "        rows.append((i, q, gold, pred_direct, \"‚úì\" if ok_direct else \"‚úó\",\n",
    "                               pred_tool,   \"‚úì\" if ok_tool   else \"‚úó\"))\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\n=== Results (No-Tool vs Tool) ===\")\n",
    "    colw = [4, 42, 8, 10, 3, 10, 3]\n",
    "    header = [\"#\", \"Question\", \"Gold\", \"No-Tool\", \"\", \"Tool\", \"\"]\n",
    "    fmt = f\"{{:<{colw[0]}}} {{:<{colw[1]}}} {{:>{colw[2]}}}  {{:>{colw[3]}}} {{:^{colw[4]}}}  {{:>{colw[5]}}} {{:^{colw[6]}}}\"\n",
    "    print(fmt.format(*header))\n",
    "    print(\"-\" * sum(colw) + \"-\"*10)\n",
    "\n",
    "    for r in rows:\n",
    "        i, q, gold, pd, okd, pt, okt = r\n",
    "        q_short = (q[:colw[1]-3] + \"‚Ä¶\") if len(q) > colw[1] else q\n",
    "        print(fmt.format(i, q_short, gold, pd, okd, pt, okt))\n",
    "\n",
    "    print(f\"\\nTotal (No-Tool): {direct_correct}/{len(questions)}\")\n",
    "    print(f\"Total (Tool)   : {tool_correct}/{len(questions)}\")\n",
    "\n",
    "#evaluate_side_by_side(QUESTIONS, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6969d6b",
   "metadata": {},
   "source": [
    "## Search section (common_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82922ae",
   "metadata": {},
   "source": [
    "Various search util methods that aren't very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import search_bing, search_brave, search_duckduck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e72b4",
   "metadata": {},
   "source": [
    "Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a2744f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install wikipedia\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "def search_wiki(query):\n",
    "    query = query.strip()\n",
    "    search_results = wikipedia.search(query, results=5)\n",
    "    print(f\"Search results: {search_results}\")\n",
    "\n",
    "    #first_result = search_results[0]\n",
    "    \n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=3)\n",
    "        print(f\"Summary: {summary}\")\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Disambiguation needed: {e.options}\")\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return f\"Page not found for: {query}\"\n",
    "    \n",
    "\n",
    "    #page = wikipedia.page(first_result)\n",
    "    #pprint(page.content)\n",
    "    \"\"\" print(f\"Title: {page.title}\")\n",
    "    print(f\"URL: {page.url}\")\n",
    "    print(f\"Content length: {len(page.content)}\")\n",
    "    print(f\"Images: {page.images[:3]}\") \"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02a560",
   "metadata": {},
   "source": [
    "Main search agent/inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "04c05894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bot(question):\n",
    "    print(question)\n",
    "\n",
    "    SYSTEM_AGENT=\"\"\"\n",
    "    You are a helpful assistant that handles searching the web for more information.\n",
    "    You respond with short and concise phrases.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Do you want to research this question?\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    (Answer YES or NO)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt2 = f\"\"\"\n",
    "    What wikipedia article should we research?\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Only respond with a single article, don't explain.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt3 = f\"\"\"\n",
    "    Given the wikipedia response given in the last chat. How would you answer this question:\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Give a short concise response.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt4 = f\"\"\"\n",
    "    If you don't want to search, then answer the question directly.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Give a short concise response.\n",
    "    \"\"\"\n",
    "    \n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_AGENT,\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    \n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    print(got)\n",
    "    \n",
    "    if got == 'YES':\n",
    "        chat = {\n",
    "            \"previous_chat\": {\n",
    "                \"prompt\": prompt,\n",
    "                \"your answer\": got\n",
    "            },\n",
    "            \"current_chat\": {\n",
    "                \"prompt\": prompt2,\n",
    "                \"your answer\": \"...\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=f\"{chat}\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        print(got)\n",
    "        chat[\"current_chat\"][\"your answer\"] = got\n",
    "        \n",
    "        res = search_wiki(got)\n",
    "        pprint(res)\n",
    "        chat[\"current_chat\"][\"wikipedia response\"] = res\n",
    "        \n",
    "        chat[\"previous_chat2\"] = chat[\"current_chat\"]\n",
    "        del chat[\"current_chat\"]\n",
    "        \n",
    "        chat[\"current_chat\"] = {\n",
    "                \"prompt\": prompt3,\n",
    "                \"your answer\": \"...\"\n",
    "            }\n",
    "        print('chat', chat)\n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=f\"{chat}\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        print(got)\n",
    "        \n",
    "    elif got == 'NO':\n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=prompt4,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        print(got)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa981fd",
   "metadata": {},
   "source": [
    "## Coding question section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6902c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_test(test, output=None):\n",
    "    display(Markdown(f\"Expected Code:\\n\\n```python\\n{test[\"expected\"]}\\n```\"))\n",
    "    if \"```python\" not in output:\n",
    "        output = f\"```python\\n{output}\\n```\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d8357ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def clean_code(code_str):\n",
    "    code_str = re.sub(r\"^```[a-zA-Z]*\\n?\", \"\", code_str)\n",
    "    code_str = re.sub(r\"\\n?```$\", \"\", code_str)\n",
    "\n",
    "    code_str = code_str.encode().decode(\"unicode_escape\")\n",
    "\n",
    "    return textwrap.dedent(code_str).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "a1c6b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import textwrap\n",
    "import sys\n",
    "\n",
    "def handle_code(code):\n",
    "    #code = textwrap.dedent(code)\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-c\", code],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=50\n",
    "    )\n",
    "\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "    \n",
    "    return result.stdout, result.stderr\n",
    "\n",
    "def get_code(test):\n",
    "    input = test[\"prompt\"]\n",
    "    SYSTEM_AGENT = \"\"\"\n",
    "    You are a python developer who writes short and quick code.capitalize\n",
    "    \n",
    "    Don't forget to call your task func with real data at the end of the code. Print the result.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt1 = f\"\"\"\n",
    "    Write the code to complete this question. Keep it short and sweet\n",
    "    \n",
    "    QUESTION: {input}\n",
    "    ---------\n",
    "    Call your task func with data at the end of the code to test it and print the result.\n",
    "    \"\"\"\n",
    "    \n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_AGENT,\n",
    "        prompt=prompt1,\n",
    "        temperature=0.2,\n",
    "        max_tokens=350\n",
    "    )\n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    got = handle_test(test, got)\n",
    "    display(Markdown(f\"\\n{got}\"))\n",
    "    \n",
    "    print_json(got)\n",
    "    cleaned = clean_code(got)\n",
    "    handle_code(cleaned)\n",
    "code = \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def solve():\n",
    "        x = np.array([1, 2, 3, 4])\n",
    "        y = x ** 2\n",
    "        return y.tolist()\n",
    "\n",
    "    print(solve())\n",
    "    \"\"\"\n",
    "#handle_code(code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00940f2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start loop\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 100\n",
      "seed: 2393\n",
      "prediction: coding\n",
      "Classes order: ['coding' 'common_sense' 'future_prediction' 'math' 'planning']\n",
      "probabilities: [9.98752478e-01 1.31286686e-04 6.83534731e-05 9.86201264e-04\n",
      " 6.16805603e-05]\n",
      "test type: coding\n",
      "question:  Perform a chi-square test of independence of variables in a contingency table. This function takes a DataFrame containing categorical data and two column names, then constructs a contingency table from the two categorical columns and performs a chi-square test of independence. It returns the p-value of the test, which indicates the probability of observing the data if the null hypothesis (independence of the variables) is true. >>> np.random.seed(42) >>> data = pd.DataFrame({ ...     'a': np.random.choice(['A', 'B'], size=100), ...     'b': np.random.choice(['X', 'Y'], size=100) ... }) >>> task_func(data, 'a', 'b') 1.0\n",
      "The function should raise the exception for: ValueError: If 'data' is empty, if 'col1' or 'col2' are not in 'data', if one or both of the columns do not have multiple categories, or if some categories have less than 5 observations (violating the chi-square test assumptions). TypeError: If one or both of the columns contain non-categorical data.\n",
      "The function should output with:\n",
      "    float: The p-value of the chi-square test of independence.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import chi2_contingency\n",
      "def task_func(data, col1, col2):\n",
      "```\n",
      "expected:     # Check if DataFrame is empty\n",
      "    if data.empty:\n",
      "        raise ValueError(\"The input DataFrame is empty.\")\n",
      "\n",
      "    # Check if specified columns exist\n",
      "    if col1 not in data or col2 not in data:\n",
      "        raise ValueError(f\"One or both of the columns '{col1}' and '{col2}' do not exist in the DataFrame.\")\n",
      "\n",
      "    # Check for non-categorical data (numerical values)\n",
      "    if np.issubdtype(data[col1].dtype, np.number) or np.issubdtype(data[col2].dtype, np.number):\n",
      "        raise TypeError(\"One or both of the columns contain non-categorical data. The chi-square test requires categorical data.\")\n",
      "\n",
      "    # Check for single category (no variability)\n",
      "    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n",
      "        raise ValueError(\"One or both of the columns do not have multiple categories. The chi-square test requires variability in data.\")\n",
      "\n",
      "    # Check for small counts in numerous categories\n",
      "    contingency_table = pd.crosstab(data[col1], data[col2])\n",
      "    if (contingency_table < 5).any().any():\n",
      "        raise ValueError(\"Some categories have less than 5 observations. This violates the assumptions of the chi-square test.\")\n",
      "\n",
      "    # Perform the chi-square test\n",
      "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
      "    return p\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Expected Code:\n",
       "\n",
       "```python\n",
       "    # Check if DataFrame is empty\n",
       "    if data.empty:\n",
       "        raise ValueError(\"The input DataFrame is empty.\")\n",
       "\n",
       "    # Check if specified columns exist\n",
       "    if col1 not in data or col2 not in data:\n",
       "        raise ValueError(f\"One or both of the columns '{col1}' and '{col2}' do not exist in the DataFrame.\")\n",
       "\n",
       "    # Check for non-categorical data (numerical values)\n",
       "    if np.issubdtype(data[col1].dtype, np.number) or np.issubdtype(data[col2].dtype, np.number):\n",
       "        raise TypeError(\"One or both of the columns contain non-categorical data. The chi-square test requires categorical data.\")\n",
       "\n",
       "    # Check for single category (no variability)\n",
       "    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n",
       "        raise ValueError(\"One or both of the columns do not have multiple categories. The chi-square test requires variability in data.\")\n",
       "\n",
       "    # Check for small counts in numerous categories\n",
       "    contingency_table = pd.crosstab(data[col1], data[col2])\n",
       "    if (contingency_table < 5).any().any():\n",
       "        raise ValueError(\"Some categories have less than 5 observations. This violates the assumptions of the chi-square test.\")\n",
       "\n",
       "    # Perform the chi-square test\n",
       "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
       "    return p\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from scipy.stats import chi2_contingency\n",
       "\n",
       "def task_func(data, col1, col2):\n",
       "    if data.empty or col1 not in data.columns or col2 not in data.columns:\n",
       "        raise ValueError(\"Invalid input data or column names\")\n",
       "    if not (data[col1].dtype == 'object' and data[col2].dtype == 'object'):\n",
       "        raise TypeError(\"Columns must contain categorical data\")\n",
       "    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n",
       "        raise ValueError(\"Columns must have multiple categories\")\n",
       "    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n",
       "        raise ValueError(\"Some categories have less than 5 observations\")\n",
       "    contingency = pd.crosstab(data[col1], data[col2])\n",
       "    chi2, p, _, _ = chi2_contingency(contingency)\n",
       "    return p\n",
       "\n",
       "# Test with sample data\n",
       "np.random.seed(42)\n",
       "data = pd.DataFrame({\n",
       "    'a': np.random.choice(['A', 'B'], size=100),\n",
       "    'b': np.random.choice(['X', 'Y'], size=100)\n",
       "})\n",
       "print(task_func(data, 'a', 'b'))\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"```python\\nimport pandas as pd\\nimport numpy as np\\nfrom scipy.stats import chi2_contingency\\n\\ndef task_func(data, col1, col2):\\n    if data.empty or col1 not in data.columns or col2 not in data.columns:\\n        raise ValueError(\\\"Invalid input data or column names\\\")\\n    if not (data[col1].dtype == 'object' and data[col2].dtype == 'object'):\\n        raise TypeError(\\\"Columns must contain categorical data\\\")\\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\\n        raise ValueError(\\\"Columns must have multiple categories\\\")\\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\\n        raise ValueError(\\\"Some categories have less than 5 observations\\\")\\n    contingency = pd.crosstab(data[col1], data[col2])\\n    chi2, p, _, _ = chi2_contingency(contingency)\\n    return p\\n\\n# Test with sample data\\nnp.random.seed(42)\\ndata = pd.DataFrame({\\n    'a': np.random.choice(['A', 'B'], size=100),\\n    'b': np.random.choice(['X', 'Y'], size=100)\\n})\\nprint(task_func(data, 'a', 'b'))\\n```\"\n",
      "1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\\nresults_llm_judge[\"seed\"] = rng\\nprint_json(results_llm_judge)\\nprint(\"\\n\",\"=\"*64)\\nload_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=True) '"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = 2393 #1741\n",
    "#seed=11789, n=30 for diverse samples\n",
    "test_prompts = get_tests(n=1, seed=rng, test_type=[\"coding\"]) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "print('seed:', rng)\n",
    "\n",
    "\n",
    "QUESTIONS = []\n",
    "for t in test_prompts:\n",
    "    ttype = classify_prompt(t[\"prompt\"])#guess_test_type(t[\"prompt\"])\n",
    "    print('test type:', ttype)\n",
    "    print('question: ', t[\"prompt\"])\n",
    "    print('expected:', t[\"expected\"])\n",
    "    QUESTIONS.append((t[\"prompt\"], t[\"expected\"], ttype))\n",
    "\n",
    "    if ttype == 'math':\n",
    "        evaluate_side_by_side(QUESTIONS, verbose=True)\n",
    "    elif ttype == 'common_sense':\n",
    "        search_bot(t[\"prompt\"])\n",
    "    elif ttype == 'coding':\n",
    "        get_code(t)\n",
    "        \"\"\" parsed = handle_test(t[\"prompt\"])\n",
    "        display(Markdown(f\"OUTPUT:\\n{parsed}\"))\n",
    "        \n",
    "        print_json(parsed) \"\"\"\n",
    "\n",
    "#load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=f\"seed: {rng}, questions: {test_prompts}\", clear=False)\n",
    "\"\"\" results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\n",
    "results_llm_judge[\"seed\"] = rng\n",
    "print_json(results_llm_judge)\n",
    "print(\"\\n\",\"=\"*64)\n",
    "load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=True) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b7ab4",
   "metadata": {},
   "source": [
    "problem seeds: 1410, 12542 (NONE), 4260 (x), 8772 (choose), 1679(factorial)\n",
    "\n",
    "15206 (wrong test), 18722(wiki cutting off n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152fb19c",
   "metadata": {},
   "source": [
    "cool working example\n",
    "\n",
    "```\n",
    "seed: 4044\n",
    "prediction: common_sense\n",
    "Classes order: ['coding' 'common_sense' 'future_prediction' 'math' 'planning']\n",
    "probabilities: [0.0938728  0.59071166 0.04256591 0.21505035 0.05779928]\n",
    "test type: common_sense\n",
    "expected: the Sumerians\n",
    "Who invented the type of script used in autographs?\n",
    "YES\n",
    "Cursive script\n",
    "Search results: ['Semi-cursive script', 'Cursive', 'Cursive script (East Asia)', 'Cursive Hebrew', 'Cursive script']\n",
    "Summary: Semi-cursive script, also known as running script, is a style of Chinese calligraphy that emerged during the Han dynasty (202 BC ‚Äì 220 AD). The style is used to write Chinese characters and is abbreviated slightly where a character's strokes are permitted to be visibly connected as the writer writes, but not to the extent of the cursive style. This makes the style easily readable by readers who can read regular script and quickly writable by calligraphers who require ideas to be written down quickly.\n",
    "('Semi-cursive script, also known as running script, is a style of Chinese '\n",
    " 'calligraphy that emerged during the Han dynasty (202 BC ‚Äì 220 AD). The style '\n",
    " 'is used to write Chinese characters and is abbreviated slightly where a '\n",
    " \"character's strokes are permitted to be visibly connected as the writer \"\n",
    " 'writes, but not to the extent of the cursive style. This makes the style '\n",
    " 'easily readable by readers who can read regular script and quickly writable '\n",
    " 'by calligraphers who require ideas to be written down quickly.')\n",
    "chat {'previous_chat': {'prompt': '\\n    Do you want to research this question?\\n\\n    QUESTION: Who invented the type of script used in autographs?\\n\\n    (Answer YES or NO)\\n    ', 'your answer': 'YES'}, 'previous_chat2': {'prompt': \"\\n    What wikipedia article should we research?\\n\\n    QUESTION: Who invented the type of script used in autographs?\\n\\n    Only respond with a single article, don't explain.\\n    \", 'your answer': 'Cursive script', 'wikipedia response': \"Semi-cursive script, also known as running script, is a style of Chinese calligraphy that emerged during the Han dynasty (202 BC ‚Äì 220 AD). The style is used to write Chinese characters and is abbreviated slightly where a character's strokes are permitted to be visibly connected as the writer writes, but not to the extent of the cursive style. This makes the style easily readable by readers who can read regular script and quickly writable by calligraphers who require ideas to be written down quickly.\"}, 'current_chat': {'prompt': '\\n    Given the wikipedia response given in the last chat. How would you answer this question:\\n\\n    QUESTION: Who invented the type of script used in autographs?\\n\\n    Give a short concise response.\\n    ', 'your answer': '...'}}\n",
    "The semi-cursive script, used in autographs, was developed during the Han dynasty.```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
