{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128,\n",
    "                                top_p: int = None,\n",
    "                                top_k: int = None,\n",
    "                                stop: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"stop\": stop\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "def load_save_json(path_in=\"parsed_dev_data.json\", path_out=None, data_in=None, clear=False):\n",
    "    data = json.load(open(path_in, \"r\", encoding=\"utf-8\")) if not clear else []\n",
    "    if path_out is not None:\n",
    "        data.append(data_in)\n",
    "        with open(path_out, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "            \n",
    "    return data\n",
    "            \n",
    "all_tests = load_save_json()\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_json(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf'), seed=None, index=None):\n",
    "    if index is not None: return all_tests[index]\n",
    "    \n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if seed is not None: random.seed(seed)\n",
    "    \n",
    "    if n == 0:\n",
    "        return [filtered_tests[start:end]]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 440\n",
      "[{'char_count': 101,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': '987',\n",
      "  'id': 'math_3_258_858',\n",
      "  'prompt': 'Find $a$ if $a$ and $b$ are integers such that $x^2 - x - 1$ is a '\n",
      "            'factor of $ax^{17} + bx^{16} + 1$ .',\n",
      "  'type': 'math'}]\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=1, upper_char=300) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output, exp):\n",
    "    exp = str(exp)\n",
    "    exp = exp.lower().strip('.')\n",
    "    out = output.lower().strip('.')\n",
    "    \n",
    "    #rare case when exp is actually yes/now and model output is true/false\n",
    "    if exp == \"yes\" and out == \"true\": return \"yes\"\n",
    "    if exp == \"no\" and out == \"false\": return \"no\"\n",
    "    \n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output, exp)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c023deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_matches(toCount, toMatch):\n",
    "    counter = Counter(toCount)\n",
    "    match_counts = {word: counter.get(word, 0) for word in toMatch}\n",
    "    total_matches = sum(match_counts.values())\n",
    "    output_len = len(toCount)\n",
    "    #print(f\"{total_matches}/{output_len} : {(total_matches / output_len) * 100 if output_len != 0 else 0}%\")\n",
    "    #print('match counts:', match_counts)\n",
    "    return total_matches, output_len\n",
    "\n",
    "def get_cosine(expected_counter, output_counter):\n",
    "    dot_product = sum(expected_counter[word] * output_counter.get(word, 0) for word in expected_counter)\n",
    "    \n",
    "    #print(f\"Dot product: {dot_product}\")\n",
    "    \n",
    "    exp_mag = math.sqrt(sum(v**2 for v in expected_counter.values()))\n",
    "    out_mag = math.sqrt(sum(v**2 for v in output_counter.values()))\n",
    "    \n",
    "    cosine_sim = 0\n",
    "    if exp_mag > 0 and out_mag > 0:\n",
    "        cosine_sim = dot_product / (exp_mag * out_mag)\n",
    "        print(f\"\\n[Cosine similarity: {cosine_sim}]\")\n",
    "        \n",
    "    return cosine_sim\n",
    "\n",
    "def get_start_end_matches(expected, output, exp_len, out_len):\n",
    "    start_matches = False\n",
    "    end_matches = False\n",
    "    if expected[0] in output[0]: start_matches = True\n",
    "    if expected[exp_len-1] in output[out_len-1]: end_matches = True\n",
    "    #print('exp', expected)\n",
    "    #print('output', output)\n",
    "    \n",
    "    #print(f\"expected[0] {expected[0]}, output[0] {output[0]}\")\n",
    "    #print(f\"expected[exp_len-1] {expected[exp_len-1]}, output[out_len-1] {output[out_len-1]}\")\n",
    "    #print(f\"START {start_matches} END {end_matches}\")\n",
    "    \n",
    "    return start_matches, end_matches\n",
    "    \n",
    "def super_match(test, output):\n",
    "    expected = str(test[\"expected\"]).replace('$', '').lower().split()\n",
    "    output = output.replace('$', '').lower().split()\n",
    "    \n",
    "    expected_counter = Counter(expected)\n",
    "    output_counter = Counter(output)\n",
    "    \n",
    "    #not very helpful in the long run...\n",
    "    get_cosine(expected_counter, output_counter)\n",
    "    \n",
    "    exp_matches, out_len = create_matches(output, expected)\n",
    "    out_matches, exp_len = create_matches(expected, output)\n",
    "    \n",
    "    return get_start_end_matches(expected, output, exp_len, out_len)\n",
    "    #return match_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "427a7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_test(test, output=None):\n",
    "    if test[\"type\"] == \"coding\":\n",
    "        display(Markdown(f\"Expected Code:\\n\\n```python\\n{test[\"expected\"]}\\n```\"))\n",
    "        if \"```python\" not in output:\n",
    "            output = f\"```python\\n{output}\\n```\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    test_samples = {\n",
    "        \"count\": len(tests),\n",
    "        \"seed\": None,\n",
    "        \"samples\": None\n",
    "    }\n",
    "    \n",
    "    for t in tests:\n",
    "        sample = {\n",
    "            \"test_count\": count,\n",
    "            \"id\": t[\"id\"],\n",
    "            \"input\": t['prompt'],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": None,\n",
    "            \"history\": {\n",
    "                \"check_correct1\": {\n",
    "                    \"match_check\": None,\n",
    "                    \"self_eval\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"no_output\": False,\n",
    "                \"truncated\": False,\n",
    "                \"check_correct2\": {\n",
    "                    \"self_eval\": None,\n",
    "                    \"self_eval2\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"check_correct3\": {\n",
    "                    \"self_eval2\": None,\n",
    "                    \"sides_matching\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"final_correctness\": None\n",
    "            }\n",
    "        }\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_json(t)\n",
    "        #handle_test(t)\n",
    "        \n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        sample[\"got\"]=got\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got, t[\"expected\"])\n",
    "        \n",
    "        has_output  = True if got != \"\" else False\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        \n",
    "        \n",
    "        got = handle_test(t, got)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        \n",
    "        print_json(got)\n",
    "        #print(got)\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        if not not_truncated:\n",
    "            #final_answers.append(False)\n",
    "            sample[\"history\"]['truncated'] = True\n",
    "            print(\"‚ùå MAX TOKENS REACHED, OUTPUT TRUNCATED, SKIPPING TESTCASE ‚ùå\")\n",
    "            continue\n",
    "        elif has_output == False:\n",
    "            sample[\"history\"]['no_output'] = True\n",
    "            print(\"‚ùå NO OUTPUT, PROMPT IS PROBABLY TOO LARGE, SKIPPING TESTCASE ‚ùå\")\n",
    "            continue\n",
    "        \n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        sample[\"history\"][\"check_correct1\"][\"match_check\"] = match_check\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        sample[\"history\"][\"check_correct1\"][\"self_eval\"] = is_correct\n",
    "        \n",
    "        seperator('\\nMODEL OUTPUT --> FIRST EVAL')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        sample[\"history\"][\"check_correct1\"]['correctness'] = correctness\n",
    "        sample[\"history\"][\"check_correct1\"]['agreement'] = agreement\n",
    "        \n",
    "        #starting and ending matches\n",
    "        #CAN BE USED TO VALIDATE SECOND MODEL, OR AS LAST RESORT\n",
    "        start_matches, end_matches = super_match(t, got)\n",
    "        sides_matching = start_matches or end_matches\n",
    "        \n",
    "        if not agreement:\n",
    "            #second model eval\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval\"] = is_correct\n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval2\"] = is_correct2\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            sample[\"history\"][\"check_correct2\"][\"correctness\"] = correctness\n",
    "            sample[\"history\"][\"check_correct2\"][\"agreement\"] = agreement\n",
    "            \n",
    "            \n",
    "            if not agreement:\n",
    "                #second model eval\n",
    "                seperator('\\nDISAGREEMENT --> THIRD EVAL')\n",
    "                print('\\nside matching:', sides_matching)\n",
    "                \n",
    "                sample[\"history\"][\"check_correct3\"][\"self_eval2\"] = is_correct2\n",
    "                sample[\"history\"][\"check_correct3\"][\"sides_matching\"] = sides_matching\n",
    "                correctness, agreement = check_correct(sides_matching, is_correct2)\n",
    "                sample[\"history\"][\"check_correct3\"][\"correctness\"] = correctness\n",
    "                sample[\"history\"][\"check_correct3\"][\"agreement\"] = agreement    \n",
    "\n",
    "\n",
    "        sample[\"history\"][\"final_correctness\"] = f\"‚úÖ {correctness}\" if correctness else f\"‚ùå {correctness}\"\n",
    "        final_answers.append(sample)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    test_samples[\"samples\"] = final_answers\n",
    "    return test_samples\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54501a74",
   "metadata": {},
   "source": [
    "Importing arithmatic solver inference time algorithm from mini lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a06deada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, textwrap, re, time, ast, operator as op\n",
    "import requests\n",
    "import math\n",
    "# --- PROVIDED: prompts ---\n",
    "SYSTEM_AGENT2 = \"\"\"You are a math tool-using agent.\n",
    "You may do exactly ONE of the following in your reply:\n",
    "1) CALCULATE: <arithmetic expression>\n",
    "   - use only numbers, + - * / **, parentheses, and round(x, ndigits)\n",
    "   - example: CALCULATE: round((3*2.49)*1.07, 2)\n",
    "2) FINAL: <answer>\n",
    "Return ONE line with the directive and value. No other text.\n",
    "\"\"\"\n",
    "SYSTEM_AGENT = \"\"\"You are a math tool-using agent.\n",
    "\n",
    "IMPORTANT: You must respond with EXACTLY ONE LINE in one of these formats:\n",
    "1) CALCULATE: <arithmetic expression>\n",
    "2) FINAL: <numeric answer>\n",
    "\n",
    "Rules:\n",
    "- NO explanations or reasoning\n",
    "- NO LaTeX or markdown\n",
    "- Arithmetic expressions can only use: numbers, +, -, *, /, **, (), round()\n",
    "- Example valid responses:\n",
    "  CALCULATE: (37 + 58) * 2\n",
    "  CALCULATE: round(23.80 * 1.15, 2)\n",
    "  FINAL: 190\n",
    "  FINAL: 27.37\n",
    "\n",
    "Respond with ONLY the directive line, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def make_first_prompt(question: str) -> str:\n",
    "    return f\"\"\"Question: {question}\n",
    "If you need arithmetic to get the answer, reply as:\n",
    "CALCULATE: <expression>\n",
    "Otherwise reply:\n",
    "FINAL: <answer>\"\"\"\n",
    "\n",
    "def make_second_prompt(result: str) -> str:\n",
    "    return f\"\"\"The calculation result is: {result}\n",
    "Now provide the final answer.\n",
    "Reply exactly as: FINAL: <answer>\"\"\"\n",
    "\n",
    "\n",
    "ACTION_RE = re.compile(r\"^\\s*(CALCULATE|FINAL)\\s*:\\s*(.+?)\\s*$\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "\n",
    "def parse_action(text: str):\n",
    "    \"\"\"\n",
    "    Returns (\"CALCULATE\", expr) or (\"FINAL\", answer); raises ValueError on bad format.\n",
    "    \"\"\"\n",
    "    # Take only the first line\n",
    "    first_line = text.strip().split('\\n')[0]\n",
    "    m = ACTION_RE.match(first_line)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unrecognized action format: {text!r}\")\n",
    "    action = m.group(1).upper()\n",
    "    payload = m.group(2).strip()\n",
    "    return action, payload\n",
    "\n",
    "# We provide this function that evaluates arithmetic expressions.\n",
    "ALLOWED_BINOPS = {ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv, ast.Pow: op.pow, ast.Mod: op.mod}\n",
    "ALLOWED_UNOPS  = {ast.UAdd: op.pos, ast.USub: op.neg}\n",
    "\n",
    "def handle_vars(expr: str, question: str):\n",
    "    #SYSTEM_AGENT = f\"\"\n",
    "\n",
    "    new_prompt = f\"\"\"\n",
    "                YOUR GOAL IS TO REPLACE VARIABLES WITH NUMBERS\n",
    "                LETTERS ARE NOT ALLOWED IN THE EXPRESSION.\n",
    "                \n",
    "                A variable(s) was detected in this expression: {expr}\n",
    "                \n",
    "                Here is the original question: {question}\n",
    "                \n",
    "                Replace each variable (letter) with a number (to the best of your ability) and return ONLY the new expression. No explanation. No letters.\n",
    "              \"\"\"\n",
    "    res = call_model_chat_completions(system=SYSTEM_AGENT, prompt=new_prompt)\n",
    "    print(\"handle vars res:\", res[\"text\"])\n",
    "    return ast.parse(res[\"text\"], mode=\"eval\")\n",
    "    \n",
    "    \n",
    "def safe_eval(expr: str, question: str):\n",
    "    \"\"\"\n",
    "    Evaluates a tiny arithmetic language: numbers, + - * / ** % parentheses, round(x, ndigits).\n",
    "    Converts '^' to '**'. Rejects anything else.\n",
    "    \"\"\"\n",
    "    expr = expr.replace(\"^\", \"**\")\n",
    "    expr = expr.replace(\"i\", \"j\")\n",
    "    if len(expr) > 300: #changed from 200 to 300.\n",
    "        raise ValueError(\"Expression too long.\")\n",
    "    node = ast.parse(expr, mode=\"eval\")\n",
    "\n",
    "    count = 0\n",
    "    def ev(n):\n",
    "        if isinstance(n, ast.Expression):  return ev(n.body)\n",
    "        if isinstance(n, ast.Constant) and isinstance(n.value, (int, float, complex)): return n.value\n",
    "        if isinstance(n, ast.UnaryOp) and type(n.op) in ALLOWED_UNOPS:        return ALLOWED_UNOPS[type(n.op)](ev(n.operand))\n",
    "        if isinstance(n, ast.BinOp) and type(n.op) in ALLOWED_BINOPS:         return ALLOWED_BINOPS[type(n.op)](ev(n.left), ev(n.right))\n",
    "        if isinstance(n, ast.Call) and isinstance(n.func, ast.Name) and n.func.id in [\"round\", \"abs\", \"sqrt\", \"ceil\"]:\n",
    "            args = [ev(a) for a in n.args]\n",
    "            if n.func.id == \"round\":\n",
    "                return round(*args)\n",
    "            elif n.func.id == \"abs\":\n",
    "                return abs(*args)\n",
    "            elif n.func.id == \"sqrt\":\n",
    "                return math.sqrt(*args)\n",
    "            elif n.func.id == \"ceil\":\n",
    "                return math.ceil(*args)\n",
    "        if isinstance(n, ast.Tuple):  # allow round(x,2) with comma\n",
    "            return tuple(ev(elt) for elt in n.elts)\n",
    "        \n",
    "        nonlocal count\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            print(f\"Disallowed expression: {ast.dump(n, include_attributes=False)}\")\n",
    "            return ev(handle_vars(expr, question))\n",
    "        raise ValueError(f\"Disallowed expression: {ast.dump(n, include_attributes=False)}\")\n",
    "\n",
    "    return ev(node)\n",
    "\n",
    "\n",
    "\n",
    "def run_agent(question: str, max_tool_uses: int = 2, verbose: bool = True):\n",
    "    # Turn 1\n",
    "    r1 = call_model_chat_completions(system=SYSTEM_AGENT, prompt=make_first_prompt(question))\n",
    "    if not r1[\"ok\"]:\n",
    "        raise RuntimeError(f\"API error: {r1['error']}\")\n",
    "    if verbose: print(\"LLM ‚Üí\", r1[\"text\"])\n",
    "    action, payload = parse_action(r1[\"text\"])\n",
    "\n",
    "    tool_uses = 0\n",
    "    while action == \"CALCULATE\":\n",
    "        if tool_uses >= max_tool_uses:\n",
    "            raise RuntimeError(\"Exceeded tool-use limit.\")\n",
    "        tool_uses += 1\n",
    "\n",
    "        calc_value = safe_eval(payload, question) #pass it question so we can handle variables\n",
    "        if verbose: print(\"CALC =\", calc_value)\n",
    "\n",
    "        # Turn 2 (+)\n",
    "        rN = call_model_chat_completions(system=SYSTEM_AGENT, prompt=make_second_prompt(str(calc_value)))\n",
    "        if not rN[\"ok\"]:\n",
    "            raise RuntimeError(f\"API error: {rN['error']}\")\n",
    "        if verbose: print(\"LLM ‚Üí\", rN[\"text\"])\n",
    "\n",
    "        action, payload = parse_action(rN[\"text\"])\n",
    "\n",
    "    # action must be FINAL here\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae6ef413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "\n",
    "# ---------- Baseline: no-tool runner ----------\n",
    "SYSTEM_DIRECT = \"You are a careful math assistant. Reply with only the final numeric answer‚Äîno explanation.\"\n",
    "\n",
    "NUM_RE = re.compile(r\"[-+]?\\d+(?:\\.\\d+)?(?:[eE][-+]?\\d+)?\")\n",
    "\n",
    "def extract_number(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to normalize model output to a single numeric string.\n",
    "    Falls back to the raw text if no number is found.\n",
    "    \"\"\"\n",
    "    m = NUM_RE.search(text)\n",
    "    num = m.group(0) if m else text.strip()\n",
    "    #print(f\"extracted num: {num}\")\n",
    "    return num\n",
    "\n",
    "def make_direct_final_prompt(result: str) -> str:\n",
    "    return f\"\"\"The calculation result is: {result}\n",
    "            Now provide the final answer, no explanation.\n",
    "            Reply exactly as: FINAL: <answer>\"\"\"\n",
    "\n",
    "def run_direct(question: str, verbose: bool = True) -> str:\n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=question,\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    if not r[\"ok\"]:\n",
    "        raise RuntimeError(f\"API error: {r['error']}\")\n",
    "    if verbose:\n",
    "        print(\"LLM(no-tool) ‚Üí\", r[\"text\"])\n",
    "        \n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=make_direct_final_prompt(r[\"text\"]),\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    print(f\"final direct: {r[\"text\"]}\")\n",
    "    return extract_number(r[\"text\"])\n",
    "\n",
    "# ---------- If you need QUESTIONS / is_correct ----------\n",
    "QUESTIONS = [\n",
    "    (\"What is (37 + 58) * 2?\", \"190\"),\n",
    "    (\"A class has 28 students; 25% are absent. How many are present?\", \"21\"),\n",
    "    (\"Solve 3x + 5 = 26 for x.\", \"7\"),\n",
    "    (\"What is 12% of 240?\", \"28.8\"),\n",
    "    (\"Average of 12, 18, 29, 31?\", \"22.5\"),\n",
    "    (\"3 notebooks cost $2.49 each, plus 7% tax. Total to 2 decimals?\", \"7.99\"),\n",
    "    (\"Convert 3.5 hours to minutes.\", \"210\"),\n",
    "    (\"Perimeter of a rectangle 8 by 11.\", \"38\"),\n",
    "    (\"What is 2.5^3?\", \"15.625\"),\n",
    "    (\"Add a 15% tip to $23.80, round to 2 decimals.\", \"27.37\"),\n",
    "]\n",
    "\n",
    "def is_correct(pred: str, gold: str):\n",
    "    try:\n",
    "        return abs(float(pred) - float(gold)) <= 1e-6\n",
    "    except:\n",
    "        return pred.strip() == gold.strip()\n",
    "\n",
    "# ---------- Evaluation harness ----------\n",
    "def evaluate_side_by_side(questions=QUESTIONS, verbose=False):\n",
    "    rows = []\n",
    "    direct_correct = 0\n",
    "    tool_correct   = 0\n",
    "\n",
    "    for i, (q, gold, ttype) in enumerate(questions, 1):\n",
    "        gold = re.findall(r\"\\d+\", gold)[-1] if len(gold) > 5 else gold\n",
    "        \n",
    "        if verbose: print(f\"\\nQ{i}: {q}\")\n",
    "\n",
    "        # No-tool\n",
    "        pred_direct = run_direct(q, verbose=verbose)\n",
    "        ok_direct   = is_correct(pred_direct, gold)\n",
    "        direct_correct += int(ok_direct)\n",
    "\n",
    "        # With tool\n",
    "        pred_tool = run_agent(q, verbose=verbose)  # uses your agent loop\n",
    "        ok_tool   = is_correct(pred_tool, gold)\n",
    "        tool_correct += int(ok_tool)\n",
    "\n",
    "        rows.append((i, q, gold, pred_direct, \"‚úì\" if ok_direct else \"‚úó\",\n",
    "                               pred_tool,   \"‚úì\" if ok_tool   else \"‚úó\"))\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\n=== Results (No-Tool vs Tool) ===\")\n",
    "    colw = [4, 42, 8, 10, 3, 10, 3]\n",
    "    header = [\"#\", \"Question\", \"Gold\", \"No-Tool\", \"\", \"Tool\", \"\"]\n",
    "    fmt = f\"{{:<{colw[0]}}} {{:<{colw[1]}}} {{:>{colw[2]}}}  {{:>{colw[3]}}} {{:^{colw[4]}}}  {{:>{colw[5]}}} {{:^{colw[6]}}}\"\n",
    "    print(fmt.format(*header))\n",
    "    print(\"-\" * sum(colw) + \"-\"*10)\n",
    "\n",
    "    for r in rows:\n",
    "        i, q, gold, pd, okd, pt, okt = r\n",
    "        q_short = (q[:colw[1]-3] + \"‚Ä¶\") if len(q) > colw[1] else q\n",
    "        print(fmt.format(i, q_short, gold, pd, okd, pt, okt))\n",
    "\n",
    "    print(f\"\\nTotal (No-Tool): {direct_correct}/{len(questions)}\")\n",
    "    print(f\"Total (Tool)   : {tool_correct}/{len(questions)}\")\n",
    "\n",
    "#evaluate_side_by_side(QUESTIONS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc2a7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs really really bad\n",
    "def guess_test_type(input, p1, p2):\n",
    "    sys = \"\"\"\n",
    "    You are a basic decider model.\n",
    "    Your goal is to distinguish between if an input sentence represents a math, common sense, planning, or coding question.\n",
    "    Respond with one word depending on the input sentence type\n",
    "    \"\"\"\n",
    "    #POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "    prompt = \"\"\"\n",
    "    What question type does the following input correspond to?\n",
    "    \n",
    "    INPUT: {input}\n",
    "    ----------------------\n",
    "    Is it a {p1} question?\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    Is it a {p2} question?\n",
    "    \n",
    "    Respond with one word\n",
    "    \"\"\"\n",
    "    return call_model_chat_completions(system=sys, prompt=prompt, temperature=0.3)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67565fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nb_classifier.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "with open('nb_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4adf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('cse_476_final_project_test_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6d1c3ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for test,i in zip(data,range(0,100)):\\n    print(test)\\n    #print(\\'ACTUAL:\\', test[\"domain\"])\\n    result = classify_prompt(test[\"input\"])\\n    print(f\"PREDICTED: {result[\\'predicted_type\\']}\")\\n    print()\\n    print(f\"Probabilities: {result[\\'probabilities\\']}\") '"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_prompt(prompt):\n",
    "    prompt_vec = vectorizer.transform([prompt])\n",
    "    \n",
    "    prediction = classifier.predict(prompt_vec)[0]\n",
    "    probabilities = classifier.predict_proba(prompt_vec)[0]\n",
    "    \n",
    "    print(f\"prediction: {prediction}\")\n",
    "    print(f\"Classes order: {classifier.classes_}\")\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    \n",
    "    result = {\n",
    "        'prompt': prompt,\n",
    "        'predicted_type': prediction,\n",
    "        'probabilities': {class_name: prob for class_name, prob in zip(classifier.classes_, probabilities)}\n",
    "    }\n",
    "    \n",
    "    high_prob_classes = [class_name for class_name, prob in zip(classifier.classes_, probabilities) if prob >= 0.40]\n",
    "    \n",
    "    if len(high_prob_classes) == 2:\n",
    "        print(f\"TWO HIGH PROB: {high_prob_classes[0]} {high_prob_classes[1]}\")\n",
    "        return guess_test_type(prompt, high_prob_classes[0], high_prob_classes[1])\n",
    "    \n",
    "    return result['predicted_type']\n",
    "\n",
    "\"\"\" for test,i in zip(data,range(0,100)):\n",
    "    print(test)\n",
    "    #print('ACTUAL:', test[\"domain\"])\n",
    "    result = classify_prompt(test[\"input\"])\n",
    "    print(f\"PREDICTED: {result['predicted_type']}\")\n",
    "    print()\n",
    "    print(f\"Probabilities: {result['probabilities']}\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e2f57",
   "metadata": {},
   "source": [
    "problem seeds: 1410, 12542 (NONE), 4260 (x), 8772 (choose), 1679(factorial)\n",
    "\n",
    "15206 (wrong test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 300\n",
      "seed: 8125\n",
      "prediction: math\n",
      "Classes order: ['coding' 'common_sense' 'future_prediction' 'math' 'planning']\n",
      "probabilities: [0.02264635 0.08787356 0.01566038 0.8584259  0.01539382]\n",
      "test type: math\n",
      "expected: 987\n",
      "\n",
      "Q1: Find $a$ if $a$ and $b$ are integers such that $x^2 - x - 1$ is a factor of $ax^{17} + bx^{16} + 1$ .\n",
      "LLM(no-tool) ‚Üí We are given that $ x^2 - x - 1 $ is a factor of $ ax^{17} + bx^{16} + 1 $, and we are to find the integer $ a $.\n",
      "\n",
      "Let $ f(x) = ax^{17} + bx^{16} + 1 $, and let $ g(x) = x^2 - x - 1 $. Since $ g(x) $ is a factor of $ f(x) $, then $ f(x) \\equiv 0 \\mod g(x) $, meaning that $ f(x) $ is divisible by $ g(x) $.\n",
      "\n",
      "Let $ \\alpha $ be a root of $ g(x) $, so $ \\alpha^2 = \\alpha + 1 $. Then $ f(\\alpha) = 0 $.\n",
      "\n",
      "So we compute:\n",
      "$$\n",
      "f(\\alpha) = a\\alpha^{17} + b\\alpha^{16} + 1 = 0\n",
      "$$\n",
      "\n",
      "We need to find $ \\alpha^{17} $ and $ \\alpha^{16} $ in terms of $ \\alpha $, using the recurrence relation from $ \\alpha^2 = \\alpha + 1 $.\n",
      "\n",
      "Let‚Äôs define a sequence $ \\alpha^n $ in terms of $ \\alpha $ and constants. Since $ \\alpha^2 = \\alpha + 1 $, we can compute higher powers of $ \\alpha $ recursively.\n",
      "\n",
      "Let‚Äôs define:\n",
      "$$\n",
      "\\alpha^n = F_n \\alpha + G_n\n",
      "$$\n",
      "Then we can compute $ F_n $ and $ G_n $ using the recurrence:\n",
      "$$\n",
      "\\alpha^{n+1} = \\alpha \\cdot \\alpha^n = \\alpha(F_n \\alpha + G_n) = F_n \\alpha^2 + G_n \\alpha = F_n(\\alpha + 1) + G_n \\alpha = (F_n + G_n)\\alpha + F_n\n",
      "$$\n",
      "So the recurrence is:\n",
      "$$\n",
      "F_{n+1} = F_n + G_n \\\\\n",
      "G_{n+1} = F_n\n",
      "$$\n",
      "With initial conditions:\n",
      "$$\n",
      "\\alpha^0 = 1 = 0\\alpha + 1 \\Rightarrow F_0 = 0, G_0 = 1 \\\\\n",
      "\\alpha^1 = \\alpha = 1\\alpha + 0 \\Rightarrow F_1 = 1, G_\n",
      "final direct: FINAL: 1\n",
      "LLM ‚Üí CALCULATE: round(1.61803398875^17, 2)\n",
      "CALC = 3571.0\n",
      "LLM ‚Üí FINAL: 3571.0\n",
      "\n",
      "=== Results (No-Tool vs Tool) ===\n",
      "#    Question                                       Gold     No-Tool            Tool    \n",
      "------------------------------------------------------------------------------------------\n",
      "1    Find $a$ if $a$ and $b$ are integers su‚Ä¶        987           1  ‚úó       3571.0  ‚úó \n",
      "\n",
      "Total (No-Tool): 0/1\n",
      "Total (Tool)   : 0/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\\nresults_llm_judge[\"seed\"] = rng\\nprint_json(results_llm_judge)\\nprint(\"\\n\",\"=\"*64)\\nload_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=True) '"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.randint(0,20000) #4105\n",
    "#seed=11789, n=30 for diverse samples\n",
    "test_prompts = get_tests(n=1, seed=rng, test_type=[\"math\"]) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "print('seed:', rng)\n",
    "QUESTIONS = []\n",
    "for t in test_prompts:\n",
    "    ttype = classify_prompt(t[\"prompt\"])#guess_test_type(t[\"prompt\"])\n",
    "    print('test type:', ttype)\n",
    "    print('expected:', t[\"expected\"])\n",
    "    QUESTIONS.append((t[\"prompt\"], t[\"expected\"], ttype))\n",
    "\n",
    "    if ttype == 'math':\n",
    "        evaluate_side_by_side(QUESTIONS, verbose=True)\n",
    "\n",
    "load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=f\"seed: {rng}, questions: {test_prompts}\", clear=False)\n",
    "\"\"\" results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\n",
    "results_llm_judge[\"seed\"] = rng\n",
    "print_json(results_llm_judge)\n",
    "print(\"\\n\",\"=\"*64)\n",
    "load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=True) \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
