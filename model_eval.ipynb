{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "all_tests = json.load(open(\"parsed_dev_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'char_count': 493,\n",
      "  'expected': '36',\n",
      "  'id': 'math_3_19_619',\n",
      "  'prompt': 'Point $D$ lies on side $\\\\overline{BC}$ of $\\\\triangle ABC$ so '\n",
      "            'that $\\\\overline{AD}$ bisects $\\\\angle BAC.$ The perpendicular '\n",
      "            'bisector of $\\\\overline{AD}$ intersects the bisectors of $\\\\angle '\n",
      "            'ABC$ and $\\\\angle ACB$ in points $E$ and $F,$ respectively. Given '\n",
      "            'that $AB=4,BC=5,$ and $CA=6,$ the area of $\\\\triangle AEF$ can be '\n",
      "            'written as $\\\\tfrac{m\\\\sqrt{n}}p,$ where $m$ and $p$ are '\n",
      "            'relatively prime positive integers, and $n$ is a positive integer '\n",
      "            'not divisible by the square of any prime. Find $m+n+p$ .',\n",
      "  'type': 'math'},\n",
      " {'char_count': 475,\n",
      "  'expected': '79',\n",
      "  'id': 'math_3_20_620',\n",
      "  'prompt': 'A hotel packed breakfast for each of three guests. Each breakfast '\n",
      "            'should have consisted of three types of rolls, one each of nut, '\n",
      "            'cheese, and fruit rolls. The preparer wrapped each of the nine '\n",
      "            'rolls and once wrapped, the rolls were indistinguishable from one '\n",
      "            'another. She then randomly put three rolls in a bag for each of '\n",
      "            'the guests. Given that the probability each guest got one roll of '\n",
      "            'each type is $\\\\frac mn,$ where $m$ and $n$ are relatively prime '\n",
      "            'integers, find $m+n.$ ',\n",
      "  'type': 'math'},\n",
      " {'char_count': 473,\n",
      "  'expected': '511',\n",
      "  'id': 'math_3_21_621',\n",
      "  'prompt': 'For all positive integers $x$ , let \\\\[f(x)=\\\\begin{cases}1 '\n",
      "            '&\\\\mbox{if }x = 1\\\\\\\\ \\\\frac x{10} &\\\\mbox{if }x\\\\mbox{ is '\n",
      "            'divisible by 10}\\\\\\\\ x+1 &\\\\mbox{otherwise}\\\\end{cases}\\\\] and '\n",
      "            'define a sequence as follows: $x_1=x$ and $x_{n+1}=f(x_n)$ for '\n",
      "            'all positive integers $n$ . Let $d(x)$ be the smallest $n$ such '\n",
      "            'that $x_n=1$ . (For example, $d(100)=3$ and $d(87)=7$ .) Let $m$ '\n",
      "            'be the number of positive integers $x$ such that $d(x)=20$ . Find '\n",
      "            'the sum of the distinct prime factors of $m$ .',\n",
      "  'type': 'math'},\n",
      " {'char_count': 470,\n",
      "  'expected': '757',\n",
      "  'id': 'math_3_22_622',\n",
      "  'prompt': 'Harold, Tanya, and Ulysses paint a very long picket fence. Harold '\n",
      "            'starts with the first picket and paints every $h$ th picket; '\n",
      "            'Tanya starts with the second picket and paints every $t$ th '\n",
      "            'picket; and Ulysses starts with the third picket and paints every '\n",
      "            '$u$ th picket. Call the positive integer $100h+10t+u$ '\n",
      "            '$\\\\textit{paintable}$ when the triple $(h,t,u)$ of positive '\n",
      "            'integers results in every picket being painted exactly once. Find '\n",
      "            'the sum of all the paintable integers.',\n",
      "  'type': 'math'},\n",
      " {'char_count': 465,\n",
      "  'expected': '560',\n",
      "  'id': 'math_3_23_623',\n",
      "  'prompt': 'In a sequence of coin tosses, one can keep a record of instances '\n",
      "            'in which a tail is immediately followed by a head, a head is '\n",
      "            'immediately followed by a head, and etc. We denote these by TH, '\n",
      "            'HH, and etc. For example, in the sequence TTTHHTHTTTHHTTH of 15 '\n",
      "            'coin tosses we observe that there are two HH, three HT, four TH, '\n",
      "            'and five TT subsequences. How many different sequences of 15 coin '\n",
      "            'tosses will contain exactly two HH, three HT, four TH, and five '\n",
      "            'TT subsequences?',\n",
      "  'type': 'math'},\n",
      " {'char_count': 461,\n",
      "  'expected': '5',\n",
      "  'id': 'math_3_24_624',\n",
      "  'prompt': 'Denali and Nate work for a dog walking business and are paid for '\n",
      "            'each dog they walk. Denali is responsible for $16$ dogs and Nate '\n",
      "            \"is responsible for $12$ dogs. Under the company's new policy, \"\n",
      "            'they will be assigned or unassigned new dogs in groups of $x$ '\n",
      "            \"dogs. The ratio of Denali's pay to Nate's pay would be the same \"\n",
      "            'if Denali started walking $4x$ more dogs and Nate stayed at $12$ '\n",
      "            \"dogs or if $x$ of Nate's dogs were reassigned to Denali. Find $x$ \"\n",
      "            'if $x\\\\neq0$.',\n",
      "  'type': 'math'},\n",
      " {'char_count': 457,\n",
      "  'expected': '25',\n",
      "  'id': 'math_3_25_625',\n",
      "  'prompt': 'In a new school, $40$ percent of the students are freshmen, $30$ '\n",
      "            'percent are sophomores, $20$ percent are juniors, and $10$ '\n",
      "            'percent are seniors. All freshmen are required to take Latin, and '\n",
      "            '$80$ percent of sophomores, $50$ percent of the juniors, and $20$ '\n",
      "            'percent of the seniors elect to take Latin. The probability that '\n",
      "            'a randomly chosen Latin student is a sophomore is $\\\\frac{m}{n}$ '\n",
      "            ', where $m$ and $n$ are relatively prime positive integers. Find '\n",
      "            '$m+n$ .',\n",
      "  'type': 'math'},\n",
      " {'char_count': 457,\n",
      "  'expected': '52',\n",
      "  'id': 'math_3_26_626',\n",
      "  'prompt': 'A container in the shape of a right circular cone is 12 inches '\n",
      "            'tall and its base has a 5-inch radius. The liquid that is sealed '\n",
      "            'inside is 9 inches deep when the cone is held with its point down '\n",
      "            'and its base horizontal. When the liquid is held with its point '\n",
      "            'up and its base horizontal, the height of the liquid is $m - '\n",
      "            'n\\\\sqrt [3]{p},$ where $m,$ $n,$ and $p$ are positive integers '\n",
      "            'and $p$ is not divisible by the cube of any prime number. Find $m '\n",
      "            '+ n + p$ .',\n",
      "  'type': 'math'},\n",
      " {'char_count': 451,\n",
      "  'expected': 'In round one, Jeff completed 16 - 1 = <<16-1=15>>15.\\n'\n",
      "              'In round two, Jeff completed 16 - 3 = <<16-3=13>>13.\\n'\n",
      "              'In round three, Jeff completed 16 + 4 = <<16+4=20>>20.\\n'\n",
      "              'In round four, Jeff completed 16 / 2 = <<16/2=8>>8.\\n'\n",
      "              'Jeff completed 15 + 13 + 20 + 8 = <<15+13+20+8=56>>56 skips in '\n",
      "              'total.\\n'\n",
      "              'Jeff skipped an average of 56 / 4 = <<56/4=14>>14 skips per '\n",
      "              'round.\\n'\n",
      "              '#### 14',\n",
      "  'id': 'math_3_27_627',\n",
      "  'prompt': 'Sam and Jeff had a skipping competition at recess. The '\n",
      "            'competition was split into four rounds. Sam completed 1 more skip '\n",
      "            'than Jeff in the first round. Jeff skipped 3 fewer times than Sam '\n",
      "            'in the second round. Jeff skipped 4 more times than Sam in the '\n",
      "            'third round. Jeff got tired and only completed half the number of '\n",
      "            'skips as Sam in the last round. If Sam skipped 16 times in each '\n",
      "            'round, what is the average number of skips per round completed by '\n",
      "            'Jeff?',\n",
      "  'type': 'math'},\n",
      " {'char_count': 448,\n",
      "  'expected': '660',\n",
      "  'id': 'math_3_28_628',\n",
      "  'prompt': 'A basketball player has a constant probability of $.4$ of making '\n",
      "            'any given shot, independent of previous shots. Let $a_n$ be the '\n",
      "            'ratio of shots made to shots attempted after $n$ shots. The '\n",
      "            'probability that $a_{10}=.4$ and $a_n\\\\le.4$ for all $n$ such '\n",
      "            'that $1\\\\le n\\\\le9$ is given to be $p^aq^br/\\\\left(s^c\\\\right)$ '\n",
      "            'where $p$ , $q$ , $r$ , and $s$ are primes, and $a$ , $b$ , and '\n",
      "            '$c$ are positive integers. Find '\n",
      "            '$\\\\left(p+q+r+s\\\\right)\\\\left(a+b+c\\\\right)$ .',\n",
      "  'type': 'math'}]\n"
     ]
    }
   ],
   "source": [
    "tests = get_random_tests(upper=400, test_type=['math']) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True HTTP: 200\n",
      "MODEL SAYS: You can use the `sympy` library in Python to find the derivative of $ y = x^2 $. Here's the code:\n",
      "\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "x = sp.symbols('x')\n",
      "y = x**2\n",
      "derivative = sp.diff(y, x)\n",
      "print(derivative)\n",
      "```\n",
      "\n",
      "The output will be: `2*x`\n"
     ]
    }
   ],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 5\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5']\n",
      "Model: 15\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5', \"previous user input: what is the previous question's answer, plus 10, previous system response: 15\"]\n",
      "Model: 150\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5', \"previous user input: what is the previous question's answer, plus 10, previous system response: 15\", \"previous user input: what is the previous question's answer, times 10, previous system response: 150\"]\n",
      "Model: 1\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5', \"previous user input: what is the previous question's answer, plus 10, previous system response: 15\", \"previous user input: what is the previous question's answer, times 10, previous system response: 150\", 'previous user input: what is the previous questions answer divided by 150?, previous system response: 1']\n",
      "Model: 155\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5', \"previous user input: what is the previous question's answer, plus 10, previous system response: 15\", \"previous user input: what is the previous question's answer, times 10, previous system response: 150\", 'previous user input: what is the previous questions answer divided by 150?, previous system response: 1', 'previous user input: What is the sum of all previous answers, previous system response: 155']\n",
      "Model: 155\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5', \"previous user input: what is the previous question's answer, plus 10, previous system response: 15\", \"previous user input: what is the previous question's answer, times 10, previous system response: 150\", 'previous user input: what is the previous questions answer divided by 150?, previous system response: 1', 'previous user input: What is the sum of all previous answers, previous system response: 155', 'previous user input: Are you sure? Check again, previous system response: 155']\n",
      "Model: 5\n",
      "['<Start of message history>', 'previous user input: what is the square root of 25, previous system response: 5', \"previous user input: what is the previous question's answer, plus 10, previous system response: 15\", \"previous user input: what is the previous question's answer, times 10, previous system response: 150\", 'previous user input: what is the previous questions answer divided by 150?, previous system response: 1', 'previous user input: What is the sum of all previous answers, previous system response: 155', 'previous user input: Are you sure? Check again, previous system response: 155', 'previous user input: No look at the first 4 answers, previous system response: 5']\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    messages = ['<Start of message history>']\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        messages.append(f\"previous user input: {user_input}, previous system response: {response['text']}\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    rows = []\n",
    "    count = 0\n",
    "    for t in tests:\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print_test(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        print(count, got)\n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        \"\"\" is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"id\": t.get(\"id\", \"<unnamed>\"),\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "        print(json.dumps(row, indent=2, ensure_ascii=False))\n",
    "        if verbose:\n",
    "            mark = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{mark} {row['id']}: expected={row['expected']!r}, got={row['got']!r} (HTTP {row['status']})\")\n",
    "            if row[\"error\"]:\n",
    "                print(\"   error:\", row[\"error\"]) \"\"\"\n",
    "\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"math_3_249_849\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"There are 25 roses in a garden. There are 40 tulips. There are 35 daisies. What percentage of flowers are not roses?\",\n",
      "  \"expected\": \"There are 25+40+35=<<25+40+35=100>>100 flowers total.\\nThere are 40+35=<<40+35=75>>75 flowers that are not roses.\\nTherefore, (75/100)*100=<<(75/100)*100=75>>75% of the flowers are not roses.\\n#### 75\",\n",
      "  \"char_count\": 116\n",
      "}\n",
      "1 75%\n",
      "{\n",
      "  \"id\": \"common_sense_1_203_303\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"What American country music singer-songwriter, born in May of 1942, sang a duet with her ex-husband the same year that he released the song \\\"The Battle?\\\"\",\n",
      "  \"expected\": \"Tammy Wynette\",\n",
      "  \"char_count\": 153\n",
      "}\n",
      "2 Dolly Parton\n",
      "{\n",
      "  \"id\": \"math_3_100_700\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Sally and Bob have made plans to go on a trip at the end of the year. They both decide to work as babysitters and save half of what they've earned for their trip. If Sally makes $6 per day and Bob makes $4 per day, how much money will they both have saved for their trip after a year?\",\n",
      "  \"expected\": \"Saly saves 1/2 * $6/day = $<<1/2*6=3>>3/day.\\nSince each year have 365 days, the total amount of money Sally will save in a year is $3/day * 365 days/year = $<<3*365=1095>>1095/year\\nBob saves 1/2 * $4/day = $<<1/2*4=2>>2/day.\\nThe total amount of money Bob will have saved in a year is $2/day * 365 days/year = $<<2*365=730>>730/year\\nIn total, Sally and Bob would have saved $730 + $1095 = $<<730+1095=1825>>1825\\n#### 1825\",\n",
      "  \"char_count\": 284\n",
      "}\n",
      "3 $600\n"
     ]
    }
   ],
   "source": [
    "results_llm_judge = self_evaluate_tests(get_random_tests(n=3, upper=300), verbose=True, model=MODEL, grader_model=MODEL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
