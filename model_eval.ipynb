{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbfe985",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Main chat completion function\n",
    "\n",
    "---\n",
    "\n",
    "Same function from tutorial, with some additions like extra hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128,\n",
    "                                top_p: int = None,\n",
    "                                top_k: int = None,\n",
    "                                stop: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"stop\": stop\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f403e",
   "metadata": {},
   "source": [
    "---\n",
    "# TESTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c16fd",
   "metadata": {},
   "source": [
    "Get all the tests, load/save, define possible types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "def load_save_json(path_in=\"parsed_dev_data.json\", path_out=None, data_in=None, clear=False):\n",
    "    data = json.load(open(path_in, \"r\", encoding=\"utf-8\")) if not clear else []\n",
    "    if path_out is not None:\n",
    "        data.append(data_in)\n",
    "        with open(path_out, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "            \n",
    "    return data\n",
    "            \n",
    "all_tests = load_save_json()\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421dedf",
   "metadata": {},
   "source": [
    "Generic get test function, mostly used for getting random tests of certain type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf'), seed=None, index=None):\n",
    "    if index is not None: return all_tests[index]\n",
    "    \n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    #print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if seed is not None: random.seed(seed)\n",
    "    \n",
    "    if n == 0:\n",
    "        return [filtered_tests[start:end]]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        #print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def interactive_chat():\\n    messages = [\"<Start of message history>\"]\\n    count = 0\\n    while True:\\n        user_input = input(\"You: \")\\n        if user_input.lower() in [\\'exit\\', \\'quit\\']:\\n            print(\"Exiting chat.\")\\n            break\\n        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\\n        count += 1\\n        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response[\\'text\\']}]\")\\n        if response[\"ok\"]:\\n            print(\"Model:\", response[\"text\"].strip())\\n        else:\\n            print(\"Error:\", response[\"error\"])\\n        print(messages) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages) \"\"\"\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60a80b",
   "metadata": {},
   "source": [
    "### Handle guessing the prompt domain/type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3400d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcd7a8",
   "metadata": {},
   "source": [
    "Terrible by itself (This qwen model is just not very capable...), might work better with this new implementation to supplement low NB confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64c8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs really really bad\n",
    "def guess_test_type(input, p1, p2):\n",
    "    sys = \"\"\"\n",
    "    You are a basic decider model.\n",
    "    Your goal is to distinguish between if an input sentence represents a math, common sense, planning, or coding question.\n",
    "    Respond with one word depending on the input sentence type\n",
    "    \"\"\"\n",
    "    #POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "    prompt = \"\"\"\n",
    "    What question type does the following input correspond to?\n",
    "    \n",
    "    INPUT: {input}\n",
    "    ----------------------\n",
    "    Is it a {p1} question?\n",
    "    \n",
    "    OR\n",
    "    \n",
    "    Is it a {p2} question?\n",
    "    \n",
    "    Respond with one word\n",
    "    \"\"\"\n",
    "    return call_model_chat_completions(system=sys, prompt=prompt, temperature=0.3)[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf38a12",
   "metadata": {},
   "source": [
    "Simple NB classifer trained in colab on the dev data. Good test accuracy around 95.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56e4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nb_classifier.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "with open('nb_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d4cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('cse_476_final_project_test_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da5607f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for test,i in zip(data,range(0,100)):\\n    print(test)\\n    #print(\\'ACTUAL:\\', test[\"domain\"])\\n    result = classify_prompt(test[\"input\"])\\n    print(f\"PREDICTED: {result[\\'predicted_type\\']}\")\\n    print()\\n    print(f\"Probabilities: {result[\\'probabilities\\']}\") '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_prompt(prompt):\n",
    "    prompt_vec = vectorizer.transform([prompt])\n",
    "    \n",
    "    prediction = classifier.predict(prompt_vec)[0]\n",
    "    probabilities = classifier.predict_proba(prompt_vec)[0]\n",
    "    \n",
    "    print(f\"prediction: {prediction}\")\n",
    "    print(f\"Classes order: {classifier.classes_}\")\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    \n",
    "    result = {\n",
    "        'prompt': prompt,\n",
    "        'predicted_type': prediction,\n",
    "        'probabilities': {class_name: prob for class_name, prob in zip(classifier.classes_, probabilities)}\n",
    "    }\n",
    "    \n",
    "    high_prob_classes = [class_name for class_name, prob in zip(classifier.classes_, probabilities) if prob >= 0.40]\n",
    "    \n",
    "    if len(high_prob_classes) == 2:\n",
    "        print(f\"TWO HIGH PROB: {high_prob_classes[0]} {high_prob_classes[1]}\")\n",
    "        return guess_test_type(prompt, high_prob_classes[0], high_prob_classes[1])\n",
    "    \n",
    "    return result['predicted_type']\n",
    "\n",
    "\"\"\" for test,i in zip(data,range(0,100)):\n",
    "    print(test)\n",
    "    #print('ACTUAL:', test[\"domain\"])\n",
    "    result = classify_prompt(test[\"input\"])\n",
    "    print(f\"PREDICTED: {result['predicted_type']}\")\n",
    "    print()\n",
    "    print(f\"Probabilities: {result['probabilities']}\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb855256",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# First Final Project Implementation\n",
    "\n",
    "---\n",
    "\n",
    "So I didn't realize we don't get expected answers on the final test data, my first implementation of the final project was based off the tutorial notebooks using expected answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba6010",
   "metadata": {},
   "source": [
    "### Self Evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8876e3",
   "metadata": {},
   "source": [
    "### Eval util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output, exp):\n",
    "    exp = str(exp)\n",
    "    exp = exp.lower().strip('.')\n",
    "    out = output.lower().strip('.')\n",
    "    \n",
    "    #rare case when exp is actually yes/now and model output is true/false\n",
    "    if exp == \"yes\" and out == \"true\": return \"yes\"\n",
    "    if exp == \"no\" and out == \"false\": return \"no\"\n",
    "    \n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output, exp)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381d948",
   "metadata": {},
   "source": [
    "Bunch of functions used when we have an expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c023deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_matches(toCount, toMatch):\n",
    "    counter = Counter(toCount)\n",
    "    match_counts = {word: counter.get(word, 0) for word in toMatch}\n",
    "    total_matches = sum(match_counts.values())\n",
    "    output_len = len(toCount)\n",
    "    #print(f\"{total_matches}/{output_len} : {(total_matches / output_len) * 100 if output_len != 0 else 0}%\")\n",
    "    #print('match counts:', match_counts)\n",
    "    return total_matches, output_len\n",
    "\n",
    "def get_cosine(expected_counter, output_counter):\n",
    "    dot_product = sum(expected_counter[word] * output_counter.get(word, 0) for word in expected_counter)\n",
    "    \n",
    "    #print(f\"Dot product: {dot_product}\")\n",
    "    \n",
    "    exp_mag = math.sqrt(sum(v**2 for v in expected_counter.values()))\n",
    "    out_mag = math.sqrt(sum(v**2 for v in output_counter.values()))\n",
    "    \n",
    "    cosine_sim = 0\n",
    "    if exp_mag > 0 and out_mag > 0:\n",
    "        cosine_sim = dot_product / (exp_mag * out_mag)\n",
    "        print(f\"\\n[Cosine similarity: {cosine_sim}]\")\n",
    "        \n",
    "    return cosine_sim\n",
    "\n",
    "def get_start_end_matches(expected, output, exp_len, out_len):\n",
    "    start_matches = False\n",
    "    end_matches = False\n",
    "    if expected[0] in output[0]: start_matches = True\n",
    "    if expected[exp_len-1] in output[out_len-1]: end_matches = True\n",
    "    #print('exp', expected)\n",
    "    #print('output', output)\n",
    "    \n",
    "    #print(f\"expected[0] {expected[0]}, output[0] {output[0]}\")\n",
    "    #print(f\"expected[exp_len-1] {expected[exp_len-1]}, output[out_len-1] {output[out_len-1]}\")\n",
    "    #print(f\"START {start_matches} END {end_matches}\")\n",
    "    \n",
    "    return start_matches, end_matches\n",
    "    \n",
    "def super_match(test, output):\n",
    "    expected = str(test[\"expected\"]).replace('$', '').lower().split()\n",
    "    output = output.replace('$', '').lower().split()\n",
    "    \n",
    "    expected_counter = Counter(expected)\n",
    "    output_counter = Counter(output)\n",
    "    \n",
    "    #not very helpful in the long run...\n",
    "    get_cosine(expected_counter, output_counter)\n",
    "    \n",
    "    exp_matches, out_len = create_matches(output, expected)\n",
    "    out_matches, exp_len = create_matches(expected, output)\n",
    "    \n",
    "    return get_start_end_matches(expected, output, exp_len, out_len)\n",
    "    #return match_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790f41d",
   "metadata": {},
   "source": [
    "### Main first implementation agent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    test_samples = {\n",
    "        \"count\": len(tests),\n",
    "        \"seed\": None,\n",
    "        \"samples\": None\n",
    "    }\n",
    "    \n",
    "    for t in tests:\n",
    "        sample = {\n",
    "            \"test_count\": count,\n",
    "            \"id\": t[\"id\"],\n",
    "            \"input\": t['prompt'],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": None,\n",
    "            \"history\": {\n",
    "                \"check_correct1\": {\n",
    "                    \"match_check\": None,\n",
    "                    \"self_eval\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"no_output\": False,\n",
    "                \"truncated\": False,\n",
    "                \"check_correct2\": {\n",
    "                    \"self_eval\": None,\n",
    "                    \"self_eval2\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"check_correct3\": {\n",
    "                    \"self_eval2\": None,\n",
    "                    \"sides_matching\": None,\n",
    "                    \"correctness\": None,\n",
    "                    \"agreement\": None\n",
    "                },\n",
    "                \"final_correctness\": None\n",
    "            }\n",
    "        }\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_json(t)\n",
    "        #handle_test(t)\n",
    "        \n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        sample[\"got\"]=got\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got, t[\"expected\"])\n",
    "        \n",
    "        has_output  = True if got != \"\" else False\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        \n",
    "        \n",
    "        got = handle_test(t, got)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        \n",
    "        print_json(got)\n",
    "        #print(got)\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        if not not_truncated:\n",
    "            #final_answers.append(False)\n",
    "            sample[\"history\"]['truncated'] = True\n",
    "            print(\"‚ùå MAX TOKENS REACHED, OUTPUT TRUNCATED, SKIPPING TESTCASE ‚ùå\")\n",
    "            continue\n",
    "        elif has_output == False:\n",
    "            sample[\"history\"]['no_output'] = True\n",
    "            print(\"‚ùå NO OUTPUT, PROMPT IS PROBABLY TOO LARGE, SKIPPING TESTCASE ‚ùå\")\n",
    "            continue\n",
    "        \n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        sample[\"history\"][\"check_correct1\"][\"match_check\"] = match_check\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        sample[\"history\"][\"check_correct1\"][\"self_eval\"] = is_correct\n",
    "        \n",
    "        seperator('\\nMODEL OUTPUT --> FIRST EVAL')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        sample[\"history\"][\"check_correct1\"]['correctness'] = correctness\n",
    "        sample[\"history\"][\"check_correct1\"]['agreement'] = agreement\n",
    "        \n",
    "        #starting and ending matches\n",
    "        #CAN BE USED TO VALIDATE SECOND MODEL, OR AS LAST RESORT\n",
    "        start_matches, end_matches = super_match(t, got)\n",
    "        sides_matching = start_matches or end_matches\n",
    "        \n",
    "        if not agreement:\n",
    "            #second model eval\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval\"] = is_correct\n",
    "            sample[\"history\"][\"check_correct2\"][\"self_eval2\"] = is_correct2\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            sample[\"history\"][\"check_correct2\"][\"correctness\"] = correctness\n",
    "            sample[\"history\"][\"check_correct2\"][\"agreement\"] = agreement\n",
    "            \n",
    "            \n",
    "            if not agreement:\n",
    "                #second model eval\n",
    "                seperator('\\nDISAGREEMENT --> THIRD EVAL')\n",
    "                print('\\nside matching:', sides_matching)\n",
    "                \n",
    "                sample[\"history\"][\"check_correct3\"][\"self_eval2\"] = is_correct2\n",
    "                sample[\"history\"][\"check_correct3\"][\"sides_matching\"] = sides_matching\n",
    "                correctness, agreement = check_correct(sides_matching, is_correct2)\n",
    "                sample[\"history\"][\"check_correct3\"][\"correctness\"] = correctness\n",
    "                sample[\"history\"][\"check_correct3\"][\"agreement\"] = agreement    \n",
    "\n",
    "\n",
    "        sample[\"history\"][\"final_correctness\"] = f\"‚úÖ {correctness}\" if correctness else f\"‚ùå {correctness}\"\n",
    "        final_answers.append(sample)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    test_samples[\"samples\"] = final_answers\n",
    "    return test_samples\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e51dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Second Final Project Implementation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e78bf",
   "metadata": {},
   "source": [
    "## Math section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54501a74",
   "metadata": {},
   "source": [
    "Importing arithmatic solver inference time algorithm from mini lab 5. This inference time agent is based off the one from mini lab 5, but also contains a ton of extra additions like more math functions and extra handling, as well as a custom inference loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a06deada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, textwrap, re, time, ast, operator as op\n",
    "import requests\n",
    "import math\n",
    "# --- PROVIDED: prompts ---\n",
    "SYSTEM_AGENT2 = \"\"\"You are a math tool-using agent.\n",
    "You may do exactly ONE of the following in your reply:\n",
    "1) CALCULATE: <arithmetic expression>\n",
    "   - use only numbers, + - * / **, parentheses, and round(x, ndigits)\n",
    "   - example: CALCULATE: round((3*2.49)*1.07, 2)\n",
    "2) FINAL: <answer>\n",
    "Return ONE line with the directive and value. No other text.\n",
    "\"\"\"\n",
    "SYSTEM_AGENT = \"\"\"You are a math tool-using agent.\n",
    "\n",
    "IMPORTANT: You must respond with EXACTLY ONE LINE in one of these formats:\n",
    "1) CALCULATE: <arithmetic expression>\n",
    "2) FINAL: <numeric answer>\n",
    "\n",
    "Rules:\n",
    "- NO explanations or reasoning\n",
    "- NO LaTeX or markdown\n",
    "- Arithmetic expressions can only use: numbers, +, -, *, /, **, (), round()\n",
    "- Example valid responses:\n",
    "  CALCULATE: (37 + 58) * 2\n",
    "  CALCULATE: round(23.80 * 1.15, 2)\n",
    "  FINAL: 190\n",
    "  FINAL: 27.37\n",
    "\n",
    "Respond with ONLY the directive line, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def make_first_prompt(question: str) -> str:\n",
    "    return f\"\"\"Question: {question}\n",
    "If you need arithmetic to get the answer, reply as:\n",
    "CALCULATE: <expression>\n",
    "Otherwise reply:\n",
    "FINAL: <answer>\"\"\"\n",
    "\n",
    "def make_second_prompt(result: str) -> str:\n",
    "    return f\"\"\"The calculation result is: {result}\n",
    "Now provide the final answer.\n",
    "Reply exactly as: FINAL: <answer>\"\"\"\n",
    "\n",
    "\n",
    "ACTION_RE = re.compile(r\"^\\s*(CALCULATE|FINAL)\\s*:\\s*(.+?)\\s*$\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "\n",
    "def parse_action(text: str):\n",
    "    \"\"\"\n",
    "    Returns (\"CALCULATE\", expr) or (\"FINAL\", answer); raises ValueError on bad format.\n",
    "    \"\"\"\n",
    "    # Take only the first line\n",
    "    first_line = text.strip().split('\\n')[0]\n",
    "    m = ACTION_RE.match(first_line)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unrecognized action format: {text!r}\")\n",
    "    action = m.group(1).upper()\n",
    "    payload = m.group(2).strip()\n",
    "    return action, payload\n",
    "\n",
    "# We provide this function that evaluates arithmetic expressions.\n",
    "ALLOWED_BINOPS = {ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv, ast.Pow: op.pow, ast.Mod: op.mod}\n",
    "ALLOWED_UNOPS  = {ast.UAdd: op.pos, ast.USub: op.neg}\n",
    "\n",
    "def handle_vars(expr: str, question: str, count):\n",
    "    #SYSTEM_AGENT = f\"\"\n",
    "\n",
    "    new_prompt = f\"\"\"\n",
    "                YOUR GOAL IS TO REPLACE VARIABLES WITH NUMBERS\n",
    "                LETTERS ARE NOT ALLOWED IN THE EXPRESSION.\n",
    "                \n",
    "                A variable(s) was detected in this expression: {expr}\n",
    "                \n",
    "                Here is the original question: {question}\n",
    "                \n",
    "                Replace each variable (letter) with a number (to the best of your ability) and return ONLY the new expression. No explanation. No letters.\n",
    "              \"\"\"\n",
    "    if count > 1:\n",
    "        new_prompt = f\"\"\"\n",
    "                    Something in the expression caused an AST parsing error.\n",
    "                    \n",
    "                    EXPRESSION {expr}\n",
    "                    \n",
    "                    Try to fix the ireggularities so data can be AST parsed. Return only the final expression. No explanation.\n",
    "                \"\"\"\n",
    "    res = call_model_chat_completions(system=SYSTEM_AGENT, prompt=new_prompt)\n",
    "    action, payload = parse_action(res[\"text\"])\n",
    "    print(\"handle vars res:\", res[\"text\"])\n",
    "    try:\n",
    "        parsed = ast.parse(payload, mode=\"eval\")\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        new_prompt2 = f\"\"\"\n",
    "                    replace expressions with their final value.\n",
    "                    \n",
    "                    for example if the expression is round(1+2+3, 0)\n",
    "                    \n",
    "                    you should return round(6, 0).\n",
    "                    \n",
    "                    EXPRESSION TO FIX: {payload}\n",
    "                    \n",
    "                    Return only the final expression. No explanation.\n",
    "                \"\"\"\n",
    "        res2 = call_model_chat_completions(system=SYSTEM_AGENT, prompt=new_prompt2)\n",
    "        action, payload = parse_action(res2[\"text\"])\n",
    "        print(\"handle vars res inner:\", payload)\n",
    "        return ast.parse(payload, mode=\"eval\")\n",
    "    \n",
    "    \n",
    "def safe_eval(expr: str, question: str):\n",
    "    \"\"\"\n",
    "    Evaluates a tiny arithmetic language: numbers, + - * / ** % parentheses, round(x, ndigits).\n",
    "    Converts '^' to '**'. Rejects anything else.\n",
    "    \"\"\"\n",
    "    expr = expr.replace(\"^\", \"**\")\n",
    "    expr = expr.replace(\"i\", \"j\")\n",
    "    if len(expr) > 300: #changed from 200 to 300.\n",
    "        raise ValueError(\"Expression too long.\")\n",
    "    node = ast.parse(expr, mode=\"eval\")\n",
    "\n",
    "    count = 0\n",
    "    def ev(n):\n",
    "        if isinstance(n, ast.Expression):  return ev(n.body)\n",
    "        if isinstance(n, ast.Constant) and isinstance(n.value, (int, float, complex)): return n.value\n",
    "        if isinstance(n, ast.UnaryOp) and type(n.op) in ALLOWED_UNOPS:        return ALLOWED_UNOPS[type(n.op)](ev(n.operand))\n",
    "        if isinstance(n, ast.BinOp) and type(n.op) in ALLOWED_BINOPS:         return ALLOWED_BINOPS[type(n.op)](ev(n.left), ev(n.right))\n",
    "        if isinstance(n, ast.Call) and isinstance(n.func, ast.Name) and n.func.id in [\"round\", \"abs\", \"sqrt\", \"ceil\"]:\n",
    "            args = [ev(a) for a in n.args]\n",
    "            if n.func.id == \"round\":\n",
    "                return round(*args)\n",
    "            elif n.func.id == \"abs\":\n",
    "                return abs(*args)\n",
    "            elif n.func.id == \"sqrt\":\n",
    "                return math.sqrt(*args)\n",
    "            elif n.func.id == \"ceil\":\n",
    "                return math.ceil(*args)\n",
    "        if isinstance(n, ast.Tuple):  # allow round(x,2) with comma\n",
    "            return tuple(ev(elt) for elt in n.elts)\n",
    "        \n",
    "        nonlocal count\n",
    "        \n",
    "        if count < 2:\n",
    "            count += 1\n",
    "            print(f\"Disallowed expression: {ast.dump(n, include_attributes=False)}\")\n",
    "            return ev(handle_vars(n, question, count))\n",
    "\n",
    "        raise ValueError(f\"Disallowed expression: {ast.dump(n, include_attributes=False)}\")\n",
    "\n",
    "    return ev(node)\n",
    "\n",
    "\n",
    "\n",
    "def run_agent(question: str, max_tool_uses: int = 2, verbose: bool = True):\n",
    "    # Turn 1\n",
    "    r1 = call_model_chat_completions(system=SYSTEM_AGENT, prompt=make_first_prompt(question))\n",
    "    if not r1[\"ok\"]:\n",
    "        raise RuntimeError(f\"API error: {r1['error']}\")\n",
    "    if verbose: print(\"LLM ‚Üí\", r1[\"text\"])\n",
    "    action, payload = parse_action(r1[\"text\"])\n",
    "\n",
    "    tool_uses = 0\n",
    "    while action == \"CALCULATE\":\n",
    "        if tool_uses >= max_tool_uses:\n",
    "            raise RuntimeError(\"Exceeded tool-use limit.\")\n",
    "        tool_uses += 1\n",
    "\n",
    "        calc_value = safe_eval(payload, question) #pass it question so we can handle variables\n",
    "        if verbose: print(\"CALC =\", calc_value)\n",
    "\n",
    "        # Turn 2 (+)\n",
    "        rN = call_model_chat_completions(system=SYSTEM_AGENT, prompt=make_second_prompt(str(calc_value)))\n",
    "        if not rN[\"ok\"]:\n",
    "            raise RuntimeError(f\"API error: {rN['error']}\")\n",
    "        if verbose: print(\"LLM ‚Üí\", rN[\"text\"])\n",
    "\n",
    "        action, payload = parse_action(rN[\"text\"])\n",
    "\n",
    "    # action must be FINAL here\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc59d54",
   "metadata": {},
   "source": [
    "Implementing the AST parser and math handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ef413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\lglen\\AppData\\Local\\Temp\\ipykernel_73760\\1727518451.py:70: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\" gold = re.findall(r\"\\d+\", gold)[-1] if len(gold) > 5 else gold\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "\n",
    "# ---------- Baseline: no-tool runner ----------\n",
    "SYSTEM_DIRECT = \"You are a careful math assistant. Reply with only the final numeric answer‚Äîno explanation.\"\n",
    "\n",
    "NUM_RE = re.compile(r\"[-+]?\\d+(?:\\.\\d+)?(?:[eE][-+]?\\d+)?\")\n",
    "\n",
    "def extract_number(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Try to normalize model output to a single numeric string.\n",
    "    Falls back to the raw text if no number is found.\n",
    "    \"\"\"\n",
    "    m = NUM_RE.search(text)\n",
    "    num = m.group(0) if m else text.strip()\n",
    "    #print(f\"extracted num: {num}\")\n",
    "    return num\n",
    "\n",
    "def make_direct_final_prompt(result: str) -> str:\n",
    "    return f\"\"\"The calculation result is: {result}\n",
    "            Now provide the final answer, no explanation.\n",
    "            Reply exactly as: FINAL: <answer>\"\"\"\n",
    "\n",
    "def run_direct(question: str, verbose: bool = True) -> str:\n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=question,\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    if not r[\"ok\"]:\n",
    "        raise RuntimeError(f\"API error: {r['error']}\")\n",
    "    if verbose:\n",
    "        print(\"LLM(no-tool) ‚Üí\", r[\"text\"])\n",
    "        \n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=make_direct_final_prompt(r[\"text\"]),\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    print(f\"final direct: {r[\"text\"]}\")\n",
    "    return extract_number(r[\"text\"])\n",
    "\n",
    "\n",
    "# ---------- Evaluation harness ----------\n",
    "def evaluate_side_by_side(input, verbose=False):\n",
    "    \"\"\" rows = []\n",
    "    direct_correct = 0\n",
    "    tool_correct   = 0 \"\"\"\n",
    "\n",
    "    #for i, (q, gold, ttype) in enumerate(questions, 1):\n",
    "    \"\"\" gold = re.findall(r\"\\d+\", gold)[-1] if len(gold) > 5 else gold\n",
    "    \n",
    "    if verbose: print(f\"\\nQ{i}: {q}\") \"\"\"\n",
    "\n",
    "    # No-tool\n",
    "    pred_direct = float(run_direct(input, verbose=verbose))\n",
    "    #ok_direct   = is_correct(pred_direct, gold)\n",
    "    #direct_correct += int(ok_direct)\n",
    "    print(f\"\\nTotal (No-Tool): {pred_direct}\")\n",
    "    # With tool\n",
    "    try:\n",
    "        pred_tool = float(run_agent(input, verbose=verbose))  # uses your agent loop\n",
    "        print(f\"Total (Tool)   : {pred_tool}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR_TYPE: {type(e).__name__} ERROR: {e}\")\n",
    "        return pred_direct\n",
    "    \n",
    "    if pred_tool == pred_direct:\n",
    "        return pred_tool\n",
    "\n",
    "    comp_prompt = f\"\"\" \n",
    "        Given this math question: {input}\n",
    "        \n",
    "        Which of these answers seems more correct?\n",
    "        \n",
    "        ANSWER 1 {pred_direct}\n",
    "        \n",
    "        ANSWER 2 {pred_tool}\n",
    "        \n",
    "        Respond with only the final answer, no explanation\n",
    "    \"\"\"\n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_DIRECT,\n",
    "        prompt=comp_prompt,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    return float(got)\n",
    "    #ok_tool   = is_correct(pred_tool, gold)\n",
    "    #tool_correct += int(ok_tool)\n",
    "\n",
    "    \"\"\" rows.append((i, q, gold, pred_direct, \"‚úì\" if ok_direct else \"‚úó\",\n",
    "                            pred_tool,   \"‚úì\" if ok_tool   else \"‚úó\"))\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\n=== Results (No-Tool vs Tool) ===\")\n",
    "    colw = [4, 42, 8, 10, 3, 10, 3]\n",
    "    header = [\"#\", \"Question\", \"Gold\", \"No-Tool\", \"\", \"Tool\", \"\"]\n",
    "    fmt = f\"{{:<{colw[0]}}} {{:<{colw[1]}}} {{:>{colw[2]}}}  {{:>{colw[3]}}} {{:^{colw[4]}}}  {{:>{colw[5]}}} {{:^{colw[6]}}}\"\n",
    "    print(fmt.format(*header))\n",
    "    print(\"-\" * sum(colw) + \"-\"*10)\n",
    "\n",
    "    for r in rows:\n",
    "        i, q, gold, pd, okd, pt, okt = r\n",
    "        q_short = (q[:colw[1]-3] + \"‚Ä¶\") if len(q) > colw[1] else q\n",
    "        print(fmt.format(i, q_short, gold, pd, okd, pt, okt)) \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#evaluate_side_by_side(QUESTIONS, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6969d6b",
   "metadata": {},
   "source": [
    "## Search section (common_sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82922ae",
   "metadata": {},
   "source": [
    "Various search util methods that aren't very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "922d007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import search_bing, search_brave, search_duckduck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e72b4",
   "metadata": {},
   "source": [
    "Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2744f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install wikipedia\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "def search_wiki(query):\n",
    "    query = query.strip()\n",
    "    search_results = wikipedia.search(query, results=5)\n",
    "    #print(f\"Search results: {search_results}\")\n",
    "\n",
    "    #first_result = search_results[0]\n",
    "    \n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=3)\n",
    "        #print(f\"Summary: {summary}\")\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Disambiguation needed: {e.options}\")\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page not found for: {query}\")\n",
    "        first_result = search_results[0]\n",
    "        summary = wikipedia.summary(first_result, sentences=3)\n",
    "        #print(f\"Summary: {summary}\")\n",
    "    \n",
    "\n",
    "    #page = wikipedia.page(first_result)\n",
    "    #pprint(page.content)\n",
    "    \"\"\" print(f\"Title: {page.title}\")\n",
    "    print(f\"URL: {page.url}\")\n",
    "    print(f\"Content length: {len(page.content)}\")\n",
    "    print(f\"Images: {page.images[:3]}\") \"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02a560",
   "metadata": {},
   "source": [
    "Main search agent/inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c05894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bot(question):\n",
    "    #print(question)\n",
    "\n",
    "    SYSTEM_AGENT=\"\"\"\n",
    "    You are a helpful assistant that handles searching the web for more information.\n",
    "    You respond with short and concise phrases.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Do you want to research this question?\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    (Answer YES or NO)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt2 = f\"\"\"\n",
    "    What wikipedia article should we research?\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Only respond with a single article, don't explain.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt3 = f\"\"\"\n",
    "    Given the wikipedia response given in the last chat. How would you answer this question:\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Give a short concise response.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt4 = f\"\"\"\n",
    "    If you don't want to search, then answer the question directly.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Give a short concise response.\n",
    "    \"\"\"\n",
    "    \n",
    "    r = call_model_chat_completions(\n",
    "        system=SYSTEM_AGENT,\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    \n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    #print(got)\n",
    "    \n",
    "    if got == 'YES':\n",
    "        chat = {\n",
    "            \"previous_chat\": {\n",
    "                \"prompt\": prompt,\n",
    "                \"your answer\": got\n",
    "            },\n",
    "            \"current_chat\": {\n",
    "                \"prompt\": prompt2,\n",
    "                \"your answer\": \"...\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=f\"{chat}\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        #print(got)\n",
    "        chat[\"current_chat\"][\"your answer\"] = got\n",
    "        \n",
    "        res = search_wiki(got)\n",
    "        #pprint(res)\n",
    "        chat[\"current_chat\"][\"wikipedia response\"] = res\n",
    "        \n",
    "        chat[\"previous_chat2\"] = chat[\"current_chat\"]\n",
    "        del chat[\"current_chat\"]\n",
    "        \n",
    "        chat[\"current_chat\"] = {\n",
    "                \"prompt\": prompt3,\n",
    "                \"your answer\": \"...\"\n",
    "            }\n",
    "        #print('chat', chat)\n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=f\"{chat}\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        #print(got)\n",
    "        \n",
    "    elif got == 'NO':\n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=prompt4,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        \n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        #print(got)\n",
    "        \n",
    "    return got\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa981fd",
   "metadata": {},
   "source": [
    "## Coding question section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6902c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_test(test, output=None):\n",
    "    display(Markdown(f\"Expected Code:\\n\\n```python\\n{test[\"expected\"]}\\n```\"))\n",
    "    if \"```python\" not in output:\n",
    "        output = f\"```python\\n{output}\\n```\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8357ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def clean_code(code_str):\n",
    "    code_str = re.sub(r\"^```[a-zA-Z]*\\n?\", \"\", code_str)\n",
    "    code_str = re.sub(r\"\\n?```$\", \"\", code_str)\n",
    "\n",
    "    code_str = code_str.encode().decode(\"unicode_escape\")\n",
    "\n",
    "    return textwrap.dedent(code_str).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1c6b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import textwrap\n",
    "import sys\n",
    "\n",
    "def handle_code(code):\n",
    "    #code = textwrap.dedent(code)\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-c\", code],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=50\n",
    "    )\n",
    "\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Code execution failed: {result.stderr}\")\n",
    "    \n",
    "    return result.stdout, result.stderr\n",
    "\n",
    "def get_code(test):\n",
    "    input = test[\"prompt\"]\n",
    "    SYSTEM_AGENT = \"\"\"\n",
    "    You are a python developer who writes short and quick code.capitalize\n",
    "    \n",
    "    Don't forget to call your task func with real data at the end of the code. Print the result.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt1 = f\"\"\"\n",
    "    Write the code to complete this question. Keep it short and sweet\n",
    "    \n",
    "    QUESTION: {input}\n",
    "    ---------\n",
    "    Call your task func with data at the end of the code to test it and print the result.\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_call(code):\n",
    "        parts = code.split('return', 1)\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        after_return = parts[1]\n",
    "        first_newline = after_return.find('\\n')\n",
    "        rest = after_return[first_newline+1:]\n",
    "        #print('TASK FUNC TEST:', rest)\n",
    "        \n",
    "        return rest\n",
    "    \n",
    "    def extract_between_backticks(text):\n",
    "        parts = text.split('```')\n",
    "        \n",
    "        if len(parts) >= 3:\n",
    "            code = parts[1]\n",
    "            \n",
    "            code = textwrap.dedent(code)\n",
    "            #print('CODE',code)\n",
    "            return code.strip()\n",
    "        \n",
    "    def handle_completion(input):\n",
    "        r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=input,\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        got = handle_test(test, got)\n",
    "        display(Markdown(f\"MODEL OUTPUT:\\n{got}\"))\n",
    "        \n",
    "        return got\n",
    "    \n",
    "    got = handle_completion(prompt1)\n",
    "    \n",
    "    #print_json(got)\n",
    "    \n",
    "    cleaned = clean_code(got)\n",
    "    task_call = extract_call(cleaned)\n",
    "    \n",
    "    try:\n",
    "        handle_code(cleaned)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR_TYPE: {type(e).__name__} ERROR: {e}\")\n",
    "        errPrompt = f\"\"\"\n",
    "            There was an error in your code!!\n",
    "            \n",
    "            Please fix and return the proper code.\n",
    "            \n",
    "            ONLY RETURN CODE NO EXPLANATIONS\n",
    "            \n",
    "            YOUR CODE: {cleaned}\n",
    "            ---------\n",
    "            ERROR_TYPE: {type(e).__name__}\n",
    "            ERROR: {e}\n",
    "        \"\"\"\n",
    "        \n",
    "        got = handle_completion(errPrompt)\n",
    "        cleaned = clean_code(got)\n",
    "        task_call = extract_call(cleaned)\n",
    "        \n",
    "        try:\n",
    "            handle_code(cleaned)\n",
    "        except:\n",
    "            pass\n",
    "    finally:\n",
    "        return cleaned    \n",
    "        \n",
    "    if test.get(\"expected\") is not None:\n",
    "        exp_header = clean_code(extract_between_backticks(input))\n",
    "        cleaned_exp = clean_code(test[\"expected\"])\n",
    "        indented_exp = '\\n'.join('    ' + line for line in cleaned_exp.splitlines())\n",
    "        \n",
    "        exp_test = f\"\"\"{exp_header}\n",
    "    {indented_exp}\n",
    "            \n",
    "        {task_call}\n",
    "        \"\"\"\n",
    "        #print(\"EXP TEST:\", exp_test)\n",
    "        handle_code(exp_test)\n",
    "\n",
    "#handle_code(code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ec5bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Planning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_planning(input):\n",
    "    SYSTEM_AGENT = f\"\"\"\n",
    "        You are a planning agent. Plan a series of instructions from the given input\n",
    "        \n",
    "        First generate your chain of thought.\n",
    "        \n",
    "        Then print \"PLANNING:\"\n",
    "        \n",
    "        Followed by each step\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        PLANNING INSTRUCTIONS: {input}\n",
    "        \n",
    "        Generate a chain of thought on how to plan each step\n",
    "        \n",
    "        Then print \"PLANNING:\"\n",
    "        \n",
    "        And add each step in order. EXAMPLE:\n",
    "        (action actuator1 object1 actuator2 object2)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=prompt,\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "    \n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    if \"PLANNING:\" in got:\n",
    "        result = got.split(\"PLANNING:\", 1)[1].strip()\n",
    "    else:\n",
    "        result = got\n",
    "    #print('GOT1:', got)\n",
    "    \n",
    "    prompt2 = f\"\"\"\n",
    "        Analyze these planning steps closely. Do they allign with the input prompt?\n",
    "        \n",
    "        PLANNING STEPS: {result}\n",
    "        -----------------------------------------------\n",
    "        INPUT PROMPT: {input}\n",
    "        \n",
    "        Generate a chain of thought, then print \"FINAL OUTPUT\"\n",
    "        \n",
    "        Give a concise answer on whether the planning steps allign with the input question\n",
    "    \"\"\"\n",
    "    \n",
    "    r = call_model_chat_completions(\n",
    "            system=SYSTEM_AGENT,\n",
    "            prompt=prompt2,\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "    \n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    #result = got.split(\"PLANNING:\", 1)[1].strip()\n",
    "    #print('GOT2:', got)\n",
    "    #return got\n",
    "    \n",
    "    prompt3 = f\"\"\"\n",
    "        Given the analysis of our planning steps:\n",
    "        \n",
    "        ANALYSIS: {got}\n",
    "        --------------------------------------\n",
    "        Update steps if need\n",
    "        \n",
    "        return the final order of steps in this format, no explanation:\n",
    "        (step 1)\n",
    "        (step 2)\n",
    "        (step n)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    r = call_model_chat_completions(\n",
    "            system=f\"You are a helpful assistant who fixes plans\",\n",
    "            prompt=prompt3,\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "    \n",
    "    got = (r.get(\"text\") or \"\").strip()\n",
    "    #result = got.split(\"PLANNING:\", 1)[1].strip()\n",
    "    #print('GOT3:', got)\n",
    "    return got\n",
    "    #return got"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00940f2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Start loop\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 9840\n",
      "prediction: planning\n",
      "Classes order: ['coding' 'common_sense' 'future_prediction' 'math' 'planning']\n",
      "probabilities: [1.18496349e-06 8.37065617e-08 3.10857857e-07 1.67177039e-06\n",
      " 9.99996749e-01]\n",
      "test type: planning\n",
      "question:  I am playing with a set of objects. Here are the actions I can do\n",
      "\n",
      "   Attack object\n",
      "   Feast object from another object\n",
      "   Succumb object\n",
      "   Overcome object from another object\n",
      "\n",
      "I have the following restrictions on my actions:\n",
      "    To perform Attack action, the following facts need to be true: Province object, Planet object, Harmony.\n",
      "    Once Attack action is performed the following facts will be true: Pain object.\n",
      "    Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.\n",
      "    To perform Succumb action, the following facts need to be true: Pain object.\n",
      "    Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.    \n",
      "    Once Succumb action is performed the following facts will be false: Pain object.\n",
      "    To perform Overcome action, the following needs to be true: Province other object, Pain object.\n",
      "    Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.\n",
      "    Once Overcome action is performed the following will be false: Province other object, Pain object.\n",
      "    To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.\n",
      "    Once Feast action is performed the following will be true: Pain object, Province other object.\n",
      "    Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, object a craves object b, harmony, planet object b, planet object c, province object a and province object c.\n",
      "My goal is to have that object a craves object c and object c craves object b.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "feast object a from object b\n",
      "succumb object a\n",
      "attack object c\n",
      "overcome object c from object b\n",
      "attack object a\n",
      "overcome object a from object c\n",
      "[PLAN END]\n",
      "\n",
      "[STATEMENT]\n",
      "As initial conditions I have that, object b craves object c, harmony, planet object a, planet object c, province object a and province object b.\n",
      "My goal is to have that object b craves object c and object c craves object a.\n",
      "\n",
      "My plan is as follows:\n",
      "\n",
      "[PLAN]\n",
      "expected: (feast b c)\n",
      "(succumb b)\n",
      "(attack c)\n",
      "(overcome c a)\n",
      "(attack b)\n",
      "(overcome b c)\n",
      "\n",
      "GOT1: Chain of Thought:\n",
      "\n",
      "1. **Initial Conditions**: We have object b craves object c, harmony, planet object a, planet object c, province object a, and province object b. The goal is to have object b craves object c and object c craves object a.\n",
      "\n",
      "2. **Goal Analysis**: We need to achieve two things:\n",
      "   - Maintain the existing relationship: object b craves object c.\n",
      "   - Create a new relationship: object c craves object a.\n",
      "\n",
      "3. **Action Selection**:\n",
      "   - **Feast object b from object c**: This action requires that \"Object Craves other object\" (object b craves object c), \"Province object\" (province object a or b), and \"Harmony\" are true. Since we already have object b craves object c, and we have province object a and b, and harmony, this action is valid.\n",
      "   - **Succumb object b**: This action requires that \"Pain object\" is true. After the Feast action, we will have \"Pain object\" as a result, so this is a valid next step.\n",
      "   - **Attack object c**: This action requires \"Province object\", \"Planet object\", and \"Harmony\". We have province object a and b, planet object a and c, and harmony. So this is valid.\n",
      "   - **Overcome object c from object a**: This action requires \"Province other object\" (province object a or b) and \"Pain object\". After the Succumb action, we will have \"Pain object\", and we have province object a and b. So this is valid.\n",
      "   - **Attack object b**: This action requires \"Province object\", \"Planet object\", and \"Harmony\". We have province object a and b, planet object a and c, and harmony. So this is valid.\n",
      "   - **Overcome object b from object c**: This action requires \"Province other object\" (province object\n",
      "GOT2: PLANNING STEPS: Chain of Thought:\n",
      "\n",
      "1. **Initial Conditions**: The initial state includes object b craves object c, harmony, planet object a, planet object c, province object a, and province object b. The goal is to maintain object b craves object c and create object c craves object a.\n",
      "\n",
      "2. **Goal Analysis**: The plan must ensure that the existing relationship (object b craves object c) is maintained while establishing a new relationship (object c craves object a).\n",
      "\n",
      "3. **Action Selection**:\n",
      "   - **Feast object b from object c**: This action requires \"Object Craves other object\" (object b craves object c), \"Province object\" (province object a or b), and \"Harmony\". These conditions are met.\n",
      "   - **Succumb object b**: This action requires \"Pain object\". After the Feast action, \"Pain object\" will be true, so this is valid.\n",
      "   - **Attack object c**: This action requires \"Province object\", \"Planet object\", and \"Harmony\". These conditions are met.\n",
      "   - **Overcome object c from object a**: This action requires \"Province other object\" (province object a or b) and \"Pain object\". After the Succumb action, \"Pain object\" will be true, and we have province object a and b, so this is valid.\n",
      "   - **Attack object b**: This action requires \"Province object\", \"Planet object\", and \"Harmony\". These conditions are met.\n",
      "   - **Overcome object b from object c**: This action requires \"Province other object\" (province object a or b) and \"Pain object\". After the Attack action, \"Pain object\" will be true, and we have province object a and b, so this is valid.\n",
      "\n",
      "PLANNING:\n",
      "1. Feast object b from object c\n",
      "2. Succumb object b\n",
      "3. Attack object c\n",
      "4\n",
      "GOT3: (Feast object b from object c)  \n",
      "(Succumb object b)  \n",
      "(Attack object c)  \n",
      "(Overcome object c from object a)  \n",
      "(Attack object b)  \n",
      "(Overcome object b from object c)\n",
      "FINAL PLANNING OUTPUT None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\\nresults_llm_judge[\"seed\"] = rng\\nprint_json(results_llm_judge)\\nprint(\"\\n\",\"=\"*64) 2861\\nload_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=True) '"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.randint(0,20000) #1741\n",
    "#seed=11789, n=30 for diverse samples\n",
    "test_prompts = get_tests(n=1, seed=rng, test_type=[\"planning\"]) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "print('seed:', rng)\n",
    "\n",
    "\n",
    "QUESTIONS = []\n",
    "for t in test_prompts:\n",
    "    ttype = classify_prompt(t[\"prompt\"])#guess_test_type(t[\"prompt\"])\n",
    "    print('test type:', ttype)\n",
    "    print('question: ', t[\"prompt\"])\n",
    "    print('expected:', t[\"expected\"])\n",
    "    QUESTIONS.append((t[\"prompt\"], t[\"expected\"], ttype))\n",
    "\n",
    "    try:\n",
    "        match ttype:\n",
    "            case 'math':\n",
    "                print('FINAL MATH OUTPUT',evaluate_side_by_side(t[\"prompt\"], verbose=True))\n",
    "            case 'common_sense':\n",
    "                print('FINAL SEARCH OUTPUT',search_bot(t[\"prompt\"]))\n",
    "            case 'coding':\n",
    "                print('FINAL CODE OUTPUT',get_code(t))\n",
    "            case 'planning':\n",
    "                print('FINAL PLANNING OUTPUT', handle_planning(t[\"prompt\"]))\n",
    "            case _:\n",
    "                print('NOT VALID TEST')\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR_TYPE: {type(e).__name__} ERROR: {e}\")\n",
    "        r = call_model_chat_completions(\n",
    "            system=\"You are a helpful assistant. Respond with short, concise answers\",\n",
    "            prompt=t[\"prompt\"],\n",
    "            temperature=0.2,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        print(f\"FALLBACK: {r['text']}\")\n",
    "        \n",
    "        \n",
    "    \"\"\" parsed = handle_test(t[\"prompt\"])\n",
    "    display(Markdown(f\"OUTPUT:\\n{parsed}\"))\n",
    "    \n",
    "    print_json(parsed) \"\"\"\n",
    "\n",
    "#load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=f\"seed: {rng}, questions: {test_prompts}\", clear=False)\n",
    "\"\"\" results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\n",
    "results_llm_judge[\"seed\"] = rng\n",
    "print_json(results_llm_judge)\n",
    "print(\"\\n\",\"=\"*64) 2861\n",
    "load_save_json(path_in=\"test_history.json\", path_out=\"test_history.json\", data_in=results_llm_judge, clear=True) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b7ab4",
   "metadata": {},
   "source": [
    "problem seeds: 1410, 12542 (NONE), 4260 (x), 8772 (choose), 1679(factorial)\n",
    "\n",
    "15206 (wrong test), 18722(wiki cutting off n), math 2506"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152fb19c",
   "metadata": {},
   "source": [
    "cool working example\n",
    "\n",
    "```\n",
    "seed: 4044\n",
    "prediction: common_sense\n",
    "Classes order: ['coding' 'common_sense' 'future_prediction' 'math' 'planning']\n",
    "probabilities: [0.0938728  0.59071166 0.04256591 0.21505035 0.05779928]\n",
    "test type: common_sense\n",
    "expected: the Sumerians\n",
    "Who invented the type of script used in autographs?\n",
    "YES\n",
    "Cursive script\n",
    "Search results: ['Semi-cursive script', 'Cursive', 'Cursive script (East Asia)', 'Cursive Hebrew', 'Cursive script']\n",
    "Summary: Semi-cursive script, also known as running script, is a style of Chinese calligraphy that emerged during the Han dynasty (202 BC ‚Äì 220 AD). The style is used to write Chinese characters and is abbreviated slightly where a character's strokes are permitted to be visibly connected as the writer writes, but not to the extent of the cursive style. This makes the style easily readable by readers who can read regular script and quickly writable by calligraphers who require ideas to be written down quickly.\n",
    "('Semi-cursive script, also known as running script, is a style of Chinese '\n",
    " 'calligraphy that emerged during the Han dynasty (202 BC ‚Äì 220 AD). The style '\n",
    " 'is used to write Chinese characters and is abbreviated slightly where a '\n",
    " \"character's strokes are permitted to be visibly connected as the writer \"\n",
    " 'writes, but not to the extent of the cursive style. This makes the style '\n",
    " 'easily readable by readers who can read regular script and quickly writable '\n",
    " 'by calligraphers who require ideas to be written down quickly.')\n",
    "chat {'previous_chat': {'prompt': '\\n    Do you want to research this question?\\n\\n    QUESTION: Who invented the type of script used in autographs?\\n\\n    (Answer YES or NO)\\n    ', 'your answer': 'YES'}, 'previous_chat2': {'prompt': \"\\n    What wikipedia article should we research?\\n\\n    QUESTION: Who invented the type of script used in autographs?\\n\\n    Only respond with a single article, don't explain.\\n    \", 'your answer': 'Cursive script', 'wikipedia response': \"Semi-cursive script, also known as running script, is a style of Chinese calligraphy that emerged during the Han dynasty (202 BC ‚Äì 220 AD). The style is used to write Chinese characters and is abbreviated slightly where a character's strokes are permitted to be visibly connected as the writer writes, but not to the extent of the cursive style. This makes the style easily readable by readers who can read regular script and quickly writable by calligraphers who require ideas to be written down quickly.\"}, 'current_chat': {'prompt': '\\n    Given the wikipedia response given in the last chat. How would you answer this question:\\n\\n    QUESTION: Who invented the type of script used in autographs?\\n\\n    Give a short concise response.\\n    ', 'your answer': '...'}}\n",
    "The semi-cursive script, used in autographs, was developed during the Han dynasty.```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
