{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "all_tests = json.load(open(\"parsed_dev_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_test(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf')):\n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if n == 0:\n",
    "        return filtered_tests[start:end]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 440\n",
      "3\n",
      "[{'char_count': 274,\n",
      "  'exp_word_count': 59,\n",
      "  'expected': '\\n'\n",
      "              '    exit_codes = []\\n'\n",
      "              '\\n'\n",
      "              '    def execute_file(file):\\n'\n",
      "              '        file_path = file\\n'\n",
      "              '        process = subprocess.Popen(file_path)\\n'\n",
      "              '        time.sleep(1)  # wait for the process to start\\n'\n",
      "              '        exit_codes.append(process.poll())  # store the exit '\n",
      "              'code\\n'\n",
      "              '\\n'\n",
      "              '    # Start a thread for each file\\n'\n",
      "              '    threads = [threading.Thread(target=execute_file, '\n",
      "              'args=(file,)) for file in file_list]\\n'\n",
      "              '    for thread in threads:\\n'\n",
      "              '        thread.start()\\n'\n",
      "              '\\n'\n",
      "              '    # Wait for all threads to finish\\n'\n",
      "              '    for thread in threads:\\n'\n",
      "              '        thread.join()\\n'\n",
      "              '\\n'\n",
      "              '    return exit_codes',\n",
      "  'id': 'coding_0_99_99',\n",
      "  'prompt': 'Run files from list of files as subprocesses at the same time.\\n'\n",
      "            'The function should output with:\\n'\n",
      "            '    list: The exit codes of the subprocesses.\\n'\n",
      "            'You should write self-contained code starting with:\\n'\n",
      "            '```\\n'\n",
      "            'import subprocess\\n'\n",
      "            'import time\\n'\n",
      "            'import threading\\n'\n",
      "            'def task_func(file_list):\\n'\n",
      "            '```',\n",
      "  'type': 'coding'},\n",
      " {'char_count': 65,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': 'Canada',\n",
      "  'id': 'common_sense_1_294_394',\n",
      "  'prompt': 'Iqaluit Airport and Canadian North are based out of what country?',\n",
      "  'type': 'common_sense'},\n",
      " {'char_count': 161,\n",
      "  'exp_word_count': 4,\n",
      "  'expected': '\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)',\n",
      "  'id': 'math_3_210_810',\n",
      "  'prompt': 'Convert the point $(0,3)$ in rectangular coordinates to polar '\n",
      "            'coordinates.  Enter your answer in the form $(r,\\\\theta),$ where '\n",
      "            '$r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
      "  'type': 'math'}]\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=-1, upper_char=300) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading an automated evaluation algorithm that returns true or false depending on the model's output.\n",
    "\n",
    "Return exactly True if the automated grading algorithm's PREDICTION on the correctness of MODEL_OUTPUT would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION (The user prompt):\n",
    "{question}\n",
    "\n",
    "MODEL_OUTPUT (Output returned from model for user prompt)\n",
    "{model_output}\n",
    "\n",
    "PREDICTION (autograder correctness output):\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER (does MODEL_OUTPUT actually contain the correct answer):\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False, depending if the autograder returned the correct grade.\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output):\n",
    "    out = output.lower().strip('.')\n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output)\n",
    "    \n",
    "    matches = re.findall(str(exp), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    rows = []\n",
    "    count = 0\n",
    "    for t in tests:\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print_test(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=400\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        got = map_tf(got)\n",
    "        display(Markdown(f\"OUTPUT: \\n{got}\"))\n",
    "        print('raw: ', got)\n",
    "        print(basic_match_check(t, got))\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"id\": t.get(\"id\", \"<unnamed>\"),\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "        print(json.dumps(row, indent=2, ensure_ascii=False))\n",
    "        if verbose:\n",
    "            mark = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{mark} {row['id']}: expected={row['expected']!r}, got={row['got']!r} (HTTP {row['status']})\")\n",
    "            if row[\"error\"]:\n",
    "                print(\"   error:\", row[\"error\"])\n",
    "\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 227\n",
      "{\n",
      "  \"id\": \"math_3_272_872\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Find the constant term in the expansion of $$\\\\left(10x^3-\\\\frac{1}{2x^2}\\\\right)^{5}$$\",\n",
      "  \"expected\": \"-125\",\n",
      "  \"char_count\": 84,\n",
      "  \"exp_word_count\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "$-\\frac{5}{32}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  $-\\frac{5}{32}$\n",
      "NO MATCH FOUND\n",
      "False\n",
      "{\n",
      "  \"id\": \"math_3_272_872\",\n",
      "  \"expected\": \"-125\",\n",
      "  \"got\": \"$-\\\\frac{5}{32}$\",\n",
      "  \"correct\": false,\n",
      "  \"status\": 200,\n",
      "  \"error\": null\n",
      "}\n",
      "❌ math_3_272_872: expected='-125', got='$-\\\\frac{5}{32}$' (HTTP 200)\n",
      "{\n",
      "  \"id\": \"math_3_299_899\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Evaluate $(1+2i)6-3i$.\",\n",
      "  \"expected\": \"6+9i\",\n",
      "  \"char_count\": 22,\n",
      "  \"exp_word_count\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "$36 + 9i$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  $36 + 9i$\n",
      "NO MATCH FOUND\n",
      "False\n",
      "{\n",
      "  \"id\": \"math_3_299_899\",\n",
      "  \"expected\": \"6+9i\",\n",
      "  \"got\": \"$36 + 9i$\",\n",
      "  \"correct\": false,\n",
      "  \"status\": 200,\n",
      "  \"error\": null\n",
      "}\n",
      "❌ math_3_299_899: expected='6+9i', got='$36 + 9i$' (HTTP 200)\n",
      "{\n",
      "  \"id\": \"math_3_216_816\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Ten points are marked on a circle. How many distinct convex polygons of three or more sides can be drawn using some (or all) of the ten points as vertices? \",\n",
      "  \"expected\": \"968\",\n",
      "  \"char_count\": 156,\n",
      "  \"exp_word_count\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "425"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  425\n",
      "NO MATCH FOUND\n",
      "False\n",
      "{\n",
      "  \"id\": \"math_3_216_816\",\n",
      "  \"expected\": \"968\",\n",
      "  \"got\": \"425\",\n",
      "  \"correct\": false,\n",
      "  \"status\": 200,\n",
      "  \"error\": null\n",
      "}\n",
      "❌ math_3_216_816: expected='968', got='425' (HTTP 200)\n",
      "{\n",
      "  \"id\": \"math_3_248_848\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"The coordinates of a parallelogram are (5, 3), (6, 8), (7, 4) and $(x, y)$ and $x > 7$. What is the value of $x + y$?\",\n",
      "  \"expected\": \"17\",\n",
      "  \"char_count\": 117,\n",
      "  \"exp_word_count\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "To find the value of $ x + y $, we use the property that the diagonals of a parallelogram bisect each other. The midpoint of the diagonals must be the same.\n",
       "\n",
       "Let the points be $ A(5, 3) $, $ B(6, 8) $, $ C(7, 4) $, and $ D(x, y) $.\n",
       "\n",
       "Assume $ A $ and $ C $ are one diagonal, and $ B $ and $ D $ are the other diagonal.\n",
       "\n",
       "Midpoint of $ AC $:  \n",
       "$$\n",
       "\\left( \\frac{5 + 7}{2}, \\frac{3 + 4}{2} \\right) = \\left( 6, \\frac{7}{2} \\right)\n",
       "$$\n",
       "\n",
       "Midpoint of $ BD $:  \n",
       "$$\n",
       "\\left( \\frac{6 + x}{2}, \\frac{8 + y}{2} \\right)\n",
       "$$\n",
       "\n",
       "Set the midpoints equal:  \n",
       "$$\n",
       "\\frac{6 + x}{2} = 6 \\quad \\text{and} \\quad \\frac{8 + y}{2} = \\frac{7}{2}\n",
       "$$\n",
       "\n",
       "Solve for $ x $:  \n",
       "$$\n",
       "6 + x = 12 \\Rightarrow x = 6\n",
       "$$\n",
       "\n",
       "Solve for $ y $:  \n",
       "$$\n",
       "8 + y = 7 \\Rightarrow y = -1\n",
       "$$\n",
       "\n",
       "Now, $ x + y = 6 + (-1) = 5 $\n",
       "\n",
       "**Answer: 5**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  To find the value of $ x + y $, we use the property that the diagonals of a parallelogram bisect each other. The midpoint of the diagonals must be the same.\n",
      "\n",
      "Let the points be $ A(5, 3) $, $ B(6, 8) $, $ C(7, 4) $, and $ D(x, y) $.\n",
      "\n",
      "Assume $ A $ and $ C $ are one diagonal, and $ B $ and $ D $ are the other diagonal.\n",
      "\n",
      "Midpoint of $ AC $:  \n",
      "$$\n",
      "\\left( \\frac{5 + 7}{2}, \\frac{3 + 4}{2} \\right) = \\left( 6, \\frac{7}{2} \\right)\n",
      "$$\n",
      "\n",
      "Midpoint of $ BD $:  \n",
      "$$\n",
      "\\left( \\frac{6 + x}{2}, \\frac{8 + y}{2} \\right)\n",
      "$$\n",
      "\n",
      "Set the midpoints equal:  \n",
      "$$\n",
      "\\frac{6 + x}{2} = 6 \\quad \\text{and} \\quad \\frac{8 + y}{2} = \\frac{7}{2}\n",
      "$$\n",
      "\n",
      "Solve for $ x $:  \n",
      "$$\n",
      "6 + x = 12 \\Rightarrow x = 6\n",
      "$$\n",
      "\n",
      "Solve for $ y $:  \n",
      "$$\n",
      "8 + y = 7 \\Rightarrow y = -1\n",
      "$$\n",
      "\n",
      "Now, $ x + y = 6 + (-1) = 5 $\n",
      "\n",
      "**Answer: 5**\n",
      "NO MATCH FOUND\n",
      "False\n",
      "{\n",
      "  \"id\": \"math_3_248_848\",\n",
      "  \"expected\": \"17\",\n",
      "  \"got\": \"To find the value of $ x + y $, we use the property that the diagonals of a parallelogram bisect each other. The midpoint of the diagonals must be the same.\\n\\nLet the points be $ A(5, 3) $, $ B(6, 8) $, $ C(7, 4) $, and $ D(x, y) $.\\n\\nAssume $ A $ and $ C $ are one diagonal, and $ B $ and $ D $ are the other diagonal.\\n\\nMidpoint of $ AC $:  \\n$$\\n\\\\left( \\\\frac{5 + 7}{2}, \\\\frac{3 + 4}{2} \\\\right) = \\\\left( 6, \\\\frac{7}{2} \\\\right)\\n$$\\n\\nMidpoint of $ BD $:  \\n$$\\n\\\\left( \\\\frac{6 + x}{2}, \\\\frac{8 + y}{2} \\\\right)\\n$$\\n\\nSet the midpoints equal:  \\n$$\\n\\\\frac{6 + x}{2} = 6 \\\\quad \\\\text{and} \\\\quad \\\\frac{8 + y}{2} = \\\\frac{7}{2}\\n$$\\n\\nSolve for $ x $:  \\n$$\\n6 + x = 12 \\\\Rightarrow x = 6\\n$$\\n\\nSolve for $ y $:  \\n$$\\n8 + y = 7 \\\\Rightarrow y = -1\\n$$\\n\\nNow, $ x + y = 6 + (-1) = 5 $\\n\\n**Answer: 5**\",\n",
      "  \"correct\": false,\n",
      "  \"status\": 200,\n",
      "  \"error\": null\n",
      "}\n",
      "❌ math_3_248_848: expected='17', got='To find the value of $ x + y $, we use the property that the diagonals of a parallelogram bisect each other. The midpoint of the diagonals must be the same.\\n\\nLet the points be $ A(5, 3) $, $ B(6, 8) $, $ C(7, 4) $, and $ D(x, y) $.\\n\\nAssume $ A $ and $ C $ are one diagonal, and $ B $ and $ D $ are the other diagonal.\\n\\nMidpoint of $ AC $:  \\n$$\\n\\\\left( \\\\frac{5 + 7}{2}, \\\\frac{3 + 4}{2} \\\\right) = \\\\left( 6, \\\\frac{7}{2} \\\\right)\\n$$\\n\\nMidpoint of $ BD $:  \\n$$\\n\\\\left( \\\\frac{6 + x}{2}, \\\\frac{8 + y}{2} \\\\right)\\n$$\\n\\nSet the midpoints equal:  \\n$$\\n\\\\frac{6 + x}{2} = 6 \\\\quad \\\\text{and} \\\\quad \\\\frac{8 + y}{2} = \\\\frac{7}{2}\\n$$\\n\\nSolve for $ x $:  \\n$$\\n6 + x = 12 \\\\Rightarrow x = 6\\n$$\\n\\nSolve for $ y $:  \\n$$\\n8 + y = 7 \\\\Rightarrow y = -1\\n$$\\n\\nNow, $ x + y = 6 + (-1) = 5 $\\n\\n**Answer: 5**' (HTTP 200)\n",
      "{\n",
      "  \"id\": \"common_sense_1_359_459\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"Would Dale Earnhardt Jr. be considered a newbie?\",\n",
      "  \"expected\": false,\n",
      "  \"char_count\": 48,\n",
      "  \"exp_word_count\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OUTPUT: \n",
       "false"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw:  false\n",
      "MATCH(ES) FOUND: ['false']\n",
      "True\n",
      "{\n",
      "  \"id\": \"common_sense_1_359_459\",\n",
      "  \"expected\": false,\n",
      "  \"got\": \"false\",\n",
      "  \"correct\": true,\n",
      "  \"status\": 200,\n",
      "  \"error\": null\n",
      "}\n",
      "✅ common_sense_1_359_459: expected=False, got='false' (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "test_prompts = get_tests(n=5, upper_exp=1, upper_char=200) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
